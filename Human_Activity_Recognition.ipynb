{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    },
    "colab": {
      "name": "Human Activity Recognition.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXEsPZZM7gEe"
      },
      "source": [
        "# Importing libraries.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from IPython.display import display\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9D78RdlHB-wJ",
        "outputId": "e7a0ec4a-b460-4824-a304-8819c80f30e9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "vJUm8eu87gEf",
        "outputId": "dcc944e9-dc32-4ff9-de15-4959478eac32"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Load up the data\n",
        "train = pd.read_csv(\"/content/drive/MyDrive/Dataset/train.csv\")\n",
        "test = pd.read_csv(\"/content/drive/MyDrive/Dataset/test.csv\")\n",
        "\n",
        "train = shuffle(train)\n",
        "test = shuffle(test)\n",
        "\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tBodyAcc-mean()-X</th>\n",
              "      <th>tBodyAcc-mean()-Y</th>\n",
              "      <th>tBodyAcc-mean()-Z</th>\n",
              "      <th>tBodyAcc-std()-X</th>\n",
              "      <th>tBodyAcc-std()-Y</th>\n",
              "      <th>tBodyAcc-std()-Z</th>\n",
              "      <th>tBodyAcc-mad()-X</th>\n",
              "      <th>tBodyAcc-mad()-Y</th>\n",
              "      <th>tBodyAcc-mad()-Z</th>\n",
              "      <th>tBodyAcc-max()-X</th>\n",
              "      <th>tBodyAcc-max()-Y</th>\n",
              "      <th>tBodyAcc-max()-Z</th>\n",
              "      <th>tBodyAcc-min()-X</th>\n",
              "      <th>tBodyAcc-min()-Y</th>\n",
              "      <th>tBodyAcc-min()-Z</th>\n",
              "      <th>tBodyAcc-sma()</th>\n",
              "      <th>tBodyAcc-energy()-X</th>\n",
              "      <th>tBodyAcc-energy()-Y</th>\n",
              "      <th>tBodyAcc-energy()-Z</th>\n",
              "      <th>tBodyAcc-iqr()-X</th>\n",
              "      <th>tBodyAcc-iqr()-Y</th>\n",
              "      <th>tBodyAcc-iqr()-Z</th>\n",
              "      <th>tBodyAcc-entropy()-X</th>\n",
              "      <th>tBodyAcc-entropy()-Y</th>\n",
              "      <th>tBodyAcc-entropy()-Z</th>\n",
              "      <th>tBodyAcc-arCoeff()-X,1</th>\n",
              "      <th>tBodyAcc-arCoeff()-X,2</th>\n",
              "      <th>tBodyAcc-arCoeff()-X,3</th>\n",
              "      <th>tBodyAcc-arCoeff()-X,4</th>\n",
              "      <th>tBodyAcc-arCoeff()-Y,1</th>\n",
              "      <th>tBodyAcc-arCoeff()-Y,2</th>\n",
              "      <th>tBodyAcc-arCoeff()-Y,3</th>\n",
              "      <th>tBodyAcc-arCoeff()-Y,4</th>\n",
              "      <th>tBodyAcc-arCoeff()-Z,1</th>\n",
              "      <th>tBodyAcc-arCoeff()-Z,2</th>\n",
              "      <th>tBodyAcc-arCoeff()-Z,3</th>\n",
              "      <th>tBodyAcc-arCoeff()-Z,4</th>\n",
              "      <th>tBodyAcc-correlation()-X,Y</th>\n",
              "      <th>tBodyAcc-correlation()-X,Z</th>\n",
              "      <th>tBodyAcc-correlation()-Y,Z</th>\n",
              "      <th>...</th>\n",
              "      <th>fBodyBodyAccJerkMag-entropy()</th>\n",
              "      <th>fBodyBodyAccJerkMag-maxInds</th>\n",
              "      <th>fBodyBodyAccJerkMag-meanFreq()</th>\n",
              "      <th>fBodyBodyAccJerkMag-skewness()</th>\n",
              "      <th>fBodyBodyAccJerkMag-kurtosis()</th>\n",
              "      <th>fBodyBodyGyroMag-mean()</th>\n",
              "      <th>fBodyBodyGyroMag-std()</th>\n",
              "      <th>fBodyBodyGyroMag-mad()</th>\n",
              "      <th>fBodyBodyGyroMag-max()</th>\n",
              "      <th>fBodyBodyGyroMag-min()</th>\n",
              "      <th>fBodyBodyGyroMag-sma()</th>\n",
              "      <th>fBodyBodyGyroMag-energy()</th>\n",
              "      <th>fBodyBodyGyroMag-iqr()</th>\n",
              "      <th>fBodyBodyGyroMag-entropy()</th>\n",
              "      <th>fBodyBodyGyroMag-maxInds</th>\n",
              "      <th>fBodyBodyGyroMag-meanFreq()</th>\n",
              "      <th>fBodyBodyGyroMag-skewness()</th>\n",
              "      <th>fBodyBodyGyroMag-kurtosis()</th>\n",
              "      <th>fBodyBodyGyroJerkMag-mean()</th>\n",
              "      <th>fBodyBodyGyroJerkMag-std()</th>\n",
              "      <th>fBodyBodyGyroJerkMag-mad()</th>\n",
              "      <th>fBodyBodyGyroJerkMag-max()</th>\n",
              "      <th>fBodyBodyGyroJerkMag-min()</th>\n",
              "      <th>fBodyBodyGyroJerkMag-sma()</th>\n",
              "      <th>fBodyBodyGyroJerkMag-energy()</th>\n",
              "      <th>fBodyBodyGyroJerkMag-iqr()</th>\n",
              "      <th>fBodyBodyGyroJerkMag-entropy()</th>\n",
              "      <th>fBodyBodyGyroJerkMag-maxInds</th>\n",
              "      <th>fBodyBodyGyroJerkMag-meanFreq()</th>\n",
              "      <th>fBodyBodyGyroJerkMag-skewness()</th>\n",
              "      <th>fBodyBodyGyroJerkMag-kurtosis()</th>\n",
              "      <th>angle(tBodyAccMean,gravity)</th>\n",
              "      <th>angle(tBodyAccJerkMean),gravityMean)</th>\n",
              "      <th>angle(tBodyGyroMean,gravityMean)</th>\n",
              "      <th>angle(tBodyGyroJerkMean,gravityMean)</th>\n",
              "      <th>angle(X,gravityMean)</th>\n",
              "      <th>angle(Y,gravityMean)</th>\n",
              "      <th>angle(Z,gravityMean)</th>\n",
              "      <th>subject</th>\n",
              "      <th>Activity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5935</th>\n",
              "      <td>0.274728</td>\n",
              "      <td>-0.014904</td>\n",
              "      <td>-0.104545</td>\n",
              "      <td>-0.997327</td>\n",
              "      <td>-0.988654</td>\n",
              "      <td>-0.992626</td>\n",
              "      <td>-0.997329</td>\n",
              "      <td>-0.987892</td>\n",
              "      <td>-0.994027</td>\n",
              "      <td>-0.945297</td>\n",
              "      <td>-0.564124</td>\n",
              "      <td>-0.807012</td>\n",
              "      <td>0.851759</td>\n",
              "      <td>0.692792</td>\n",
              "      <td>0.846525</td>\n",
              "      <td>-0.995213</td>\n",
              "      <td>-0.999984</td>\n",
              "      <td>-0.999923</td>\n",
              "      <td>-0.999856</td>\n",
              "      <td>-0.997181</td>\n",
              "      <td>-0.988126</td>\n",
              "      <td>-0.995713</td>\n",
              "      <td>-0.736298</td>\n",
              "      <td>-0.541497</td>\n",
              "      <td>-0.517509</td>\n",
              "      <td>0.229928</td>\n",
              "      <td>-0.203782</td>\n",
              "      <td>0.223076</td>\n",
              "      <td>-0.040335</td>\n",
              "      <td>0.331924</td>\n",
              "      <td>-0.253588</td>\n",
              "      <td>0.226645</td>\n",
              "      <td>0.124380</td>\n",
              "      <td>0.513717</td>\n",
              "      <td>-0.268387</td>\n",
              "      <td>0.111120</td>\n",
              "      <td>-0.131839</td>\n",
              "      <td>-0.473562</td>\n",
              "      <td>-0.294682</td>\n",
              "      <td>0.329094</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-0.619048</td>\n",
              "      <td>0.446406</td>\n",
              "      <td>-0.631868</td>\n",
              "      <td>-0.898361</td>\n",
              "      <td>-0.997758</td>\n",
              "      <td>-0.997082</td>\n",
              "      <td>-0.997366</td>\n",
              "      <td>-0.996741</td>\n",
              "      <td>-0.996955</td>\n",
              "      <td>-0.997758</td>\n",
              "      <td>-0.999986</td>\n",
              "      <td>-0.997026</td>\n",
              "      <td>-0.956057</td>\n",
              "      <td>-0.846154</td>\n",
              "      <td>0.165948</td>\n",
              "      <td>-0.388509</td>\n",
              "      <td>-0.692234</td>\n",
              "      <td>-0.999354</td>\n",
              "      <td>-0.999118</td>\n",
              "      <td>-0.998894</td>\n",
              "      <td>-0.999536</td>\n",
              "      <td>-0.998747</td>\n",
              "      <td>-0.999354</td>\n",
              "      <td>-0.999998</td>\n",
              "      <td>-0.998068</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-0.714286</td>\n",
              "      <td>0.545741</td>\n",
              "      <td>-0.768042</td>\n",
              "      <td>-0.948903</td>\n",
              "      <td>0.077922</td>\n",
              "      <td>-0.071491</td>\n",
              "      <td>0.262907</td>\n",
              "      <td>-0.145298</td>\n",
              "      <td>-0.774787</td>\n",
              "      <td>0.017287</td>\n",
              "      <td>-0.123195</td>\n",
              "      <td>27</td>\n",
              "      <td>SITTING</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1076</th>\n",
              "      <td>0.275425</td>\n",
              "      <td>-0.018261</td>\n",
              "      <td>-0.147606</td>\n",
              "      <td>-0.963274</td>\n",
              "      <td>-0.923889</td>\n",
              "      <td>-0.960557</td>\n",
              "      <td>-0.971405</td>\n",
              "      <td>-0.943127</td>\n",
              "      <td>-0.961636</td>\n",
              "      <td>-0.889782</td>\n",
              "      <td>-0.483282</td>\n",
              "      <td>-0.817666</td>\n",
              "      <td>0.804425</td>\n",
              "      <td>0.626119</td>\n",
              "      <td>0.805364</td>\n",
              "      <td>-0.948523</td>\n",
              "      <td>-0.999171</td>\n",
              "      <td>-0.998607</td>\n",
              "      <td>-0.996481</td>\n",
              "      <td>-0.983188</td>\n",
              "      <td>-0.969028</td>\n",
              "      <td>-0.961622</td>\n",
              "      <td>-0.224517</td>\n",
              "      <td>-0.364426</td>\n",
              "      <td>-0.918056</td>\n",
              "      <td>0.218828</td>\n",
              "      <td>-0.135106</td>\n",
              "      <td>0.293957</td>\n",
              "      <td>0.052992</td>\n",
              "      <td>0.259501</td>\n",
              "      <td>-0.023626</td>\n",
              "      <td>0.248777</td>\n",
              "      <td>0.284053</td>\n",
              "      <td>0.054935</td>\n",
              "      <td>-0.016770</td>\n",
              "      <td>0.020409</td>\n",
              "      <td>-0.205066</td>\n",
              "      <td>0.354079</td>\n",
              "      <td>-0.384339</td>\n",
              "      <td>-0.116800</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.723642</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-0.023906</td>\n",
              "      <td>-0.277928</td>\n",
              "      <td>-0.724619</td>\n",
              "      <td>-0.896079</td>\n",
              "      <td>-0.908309</td>\n",
              "      <td>-0.885202</td>\n",
              "      <td>-0.929791</td>\n",
              "      <td>-0.980017</td>\n",
              "      <td>-0.896079</td>\n",
              "      <td>-0.993855</td>\n",
              "      <td>-0.879002</td>\n",
              "      <td>-0.058818</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-0.085951</td>\n",
              "      <td>-0.626073</td>\n",
              "      <td>-0.878264</td>\n",
              "      <td>-0.934920</td>\n",
              "      <td>-0.934916</td>\n",
              "      <td>-0.928180</td>\n",
              "      <td>-0.943941</td>\n",
              "      <td>-0.943712</td>\n",
              "      <td>-0.934920</td>\n",
              "      <td>-0.997563</td>\n",
              "      <td>-0.935072</td>\n",
              "      <td>-0.405872</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.008401</td>\n",
              "      <td>-0.277661</td>\n",
              "      <td>-0.690014</td>\n",
              "      <td>0.005052</td>\n",
              "      <td>-0.159060</td>\n",
              "      <td>0.478429</td>\n",
              "      <td>-0.759881</td>\n",
              "      <td>0.603273</td>\n",
              "      <td>-0.971333</td>\n",
              "      <td>0.051999</td>\n",
              "      <td>6</td>\n",
              "      <td>LAYING</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1445</th>\n",
              "      <td>0.318558</td>\n",
              "      <td>-0.072246</td>\n",
              "      <td>-0.130682</td>\n",
              "      <td>0.115513</td>\n",
              "      <td>-0.134942</td>\n",
              "      <td>0.254054</td>\n",
              "      <td>0.005706</td>\n",
              "      <td>-0.176049</td>\n",
              "      <td>0.181079</td>\n",
              "      <td>0.557096</td>\n",
              "      <td>-0.089706</td>\n",
              "      <td>-0.355682</td>\n",
              "      <td>0.164939</td>\n",
              "      <td>0.194947</td>\n",
              "      <td>-0.327351</td>\n",
              "      <td>0.108383</td>\n",
              "      <td>-0.377011</td>\n",
              "      <td>-0.849320</td>\n",
              "      <td>-0.299110</td>\n",
              "      <td>-0.313223</td>\n",
              "      <td>-0.434255</td>\n",
              "      <td>-0.126833</td>\n",
              "      <td>0.072865</td>\n",
              "      <td>0.156028</td>\n",
              "      <td>0.445398</td>\n",
              "      <td>-0.548681</td>\n",
              "      <td>0.506811</td>\n",
              "      <td>-0.279230</td>\n",
              "      <td>0.173780</td>\n",
              "      <td>-0.313484</td>\n",
              "      <td>0.278858</td>\n",
              "      <td>0.044133</td>\n",
              "      <td>0.001505</td>\n",
              "      <td>-0.540331</td>\n",
              "      <td>0.553381</td>\n",
              "      <td>-0.484311</td>\n",
              "      <td>0.239780</td>\n",
              "      <td>-0.596925</td>\n",
              "      <td>-0.808827</td>\n",
              "      <td>0.656463</td>\n",
              "      <td>...</td>\n",
              "      <td>0.722962</td>\n",
              "      <td>-0.873016</td>\n",
              "      <td>-0.043584</td>\n",
              "      <td>-0.024476</td>\n",
              "      <td>-0.332238</td>\n",
              "      <td>-0.222275</td>\n",
              "      <td>-0.269100</td>\n",
              "      <td>-0.357779</td>\n",
              "      <td>-0.129712</td>\n",
              "      <td>-0.519809</td>\n",
              "      <td>-0.222275</td>\n",
              "      <td>-0.649862</td>\n",
              "      <td>-0.361073</td>\n",
              "      <td>0.638150</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.239161</td>\n",
              "      <td>0.245617</td>\n",
              "      <td>0.014827</td>\n",
              "      <td>-0.188165</td>\n",
              "      <td>-0.238862</td>\n",
              "      <td>-0.108602</td>\n",
              "      <td>-0.287391</td>\n",
              "      <td>-0.359858</td>\n",
              "      <td>-0.188165</td>\n",
              "      <td>-0.666470</td>\n",
              "      <td>-0.089026</td>\n",
              "      <td>0.682719</td>\n",
              "      <td>-0.873016</td>\n",
              "      <td>0.081290</td>\n",
              "      <td>-0.267270</td>\n",
              "      <td>-0.618729</td>\n",
              "      <td>-0.274114</td>\n",
              "      <td>-0.105113</td>\n",
              "      <td>0.974814</td>\n",
              "      <td>0.163274</td>\n",
              "      <td>-0.676483</td>\n",
              "      <td>0.166712</td>\n",
              "      <td>0.239572</td>\n",
              "      <td>7</td>\n",
              "      <td>WALKING_DOWNSTAIRS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3817</th>\n",
              "      <td>0.276492</td>\n",
              "      <td>-0.007673</td>\n",
              "      <td>-0.095866</td>\n",
              "      <td>-0.976019</td>\n",
              "      <td>-0.808805</td>\n",
              "      <td>-0.870906</td>\n",
              "      <td>-0.978138</td>\n",
              "      <td>-0.806568</td>\n",
              "      <td>-0.863624</td>\n",
              "      <td>-0.921992</td>\n",
              "      <td>-0.464084</td>\n",
              "      <td>-0.761480</td>\n",
              "      <td>0.826486</td>\n",
              "      <td>0.632910</td>\n",
              "      <td>0.796293</td>\n",
              "      <td>-0.898818</td>\n",
              "      <td>-0.999611</td>\n",
              "      <td>-0.992172</td>\n",
              "      <td>-0.990671</td>\n",
              "      <td>-0.983171</td>\n",
              "      <td>-0.848258</td>\n",
              "      <td>-0.837546</td>\n",
              "      <td>-0.277912</td>\n",
              "      <td>-0.035797</td>\n",
              "      <td>-0.157838</td>\n",
              "      <td>-0.080384</td>\n",
              "      <td>0.176814</td>\n",
              "      <td>0.052146</td>\n",
              "      <td>-0.047606</td>\n",
              "      <td>-0.519320</td>\n",
              "      <td>0.283047</td>\n",
              "      <td>0.168913</td>\n",
              "      <td>-0.176645</td>\n",
              "      <td>0.055766</td>\n",
              "      <td>-0.074649</td>\n",
              "      <td>-0.090818</td>\n",
              "      <td>0.065018</td>\n",
              "      <td>-0.258565</td>\n",
              "      <td>-0.421272</td>\n",
              "      <td>0.758457</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.898000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-0.060761</td>\n",
              "      <td>-0.294762</td>\n",
              "      <td>-0.673157</td>\n",
              "      <td>-0.922069</td>\n",
              "      <td>-0.855269</td>\n",
              "      <td>-0.875273</td>\n",
              "      <td>-0.863877</td>\n",
              "      <td>-0.986881</td>\n",
              "      <td>-0.922069</td>\n",
              "      <td>-0.990302</td>\n",
              "      <td>-0.931939</td>\n",
              "      <td>-0.344838</td>\n",
              "      <td>-0.897436</td>\n",
              "      <td>-0.663943</td>\n",
              "      <td>0.140887</td>\n",
              "      <td>-0.269326</td>\n",
              "      <td>-0.973487</td>\n",
              "      <td>-0.967297</td>\n",
              "      <td>-0.963927</td>\n",
              "      <td>-0.973271</td>\n",
              "      <td>-0.984547</td>\n",
              "      <td>-0.973487</td>\n",
              "      <td>-0.999459</td>\n",
              "      <td>-0.959676</td>\n",
              "      <td>-0.585399</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-0.353601</td>\n",
              "      <td>-0.173079</td>\n",
              "      <td>-0.592688</td>\n",
              "      <td>0.045424</td>\n",
              "      <td>0.109566</td>\n",
              "      <td>0.780737</td>\n",
              "      <td>-0.669733</td>\n",
              "      <td>-0.770001</td>\n",
              "      <td>0.212463</td>\n",
              "      <td>-0.096674</td>\n",
              "      <td>19</td>\n",
              "      <td>STANDING</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1961</th>\n",
              "      <td>0.279312</td>\n",
              "      <td>-0.016641</td>\n",
              "      <td>-0.109667</td>\n",
              "      <td>-0.997606</td>\n",
              "      <td>-0.996191</td>\n",
              "      <td>-0.992375</td>\n",
              "      <td>-0.997717</td>\n",
              "      <td>-0.995647</td>\n",
              "      <td>-0.992080</td>\n",
              "      <td>-0.941896</td>\n",
              "      <td>-0.574756</td>\n",
              "      <td>-0.822200</td>\n",
              "      <td>0.851336</td>\n",
              "      <td>0.695283</td>\n",
              "      <td>0.843193</td>\n",
              "      <td>-0.997567</td>\n",
              "      <td>-0.999987</td>\n",
              "      <td>-0.999982</td>\n",
              "      <td>-0.999869</td>\n",
              "      <td>-0.997251</td>\n",
              "      <td>-0.994537</td>\n",
              "      <td>-0.990989</td>\n",
              "      <td>-0.647231</td>\n",
              "      <td>-0.727343</td>\n",
              "      <td>-0.611503</td>\n",
              "      <td>0.185806</td>\n",
              "      <td>-0.005369</td>\n",
              "      <td>-0.122806</td>\n",
              "      <td>0.321039</td>\n",
              "      <td>0.264502</td>\n",
              "      <td>-0.112822</td>\n",
              "      <td>0.194116</td>\n",
              "      <td>0.180082</td>\n",
              "      <td>0.614778</td>\n",
              "      <td>-0.323595</td>\n",
              "      <td>0.281964</td>\n",
              "      <td>-0.027220</td>\n",
              "      <td>-0.331531</td>\n",
              "      <td>0.086230</td>\n",
              "      <td>-0.069054</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-0.809524</td>\n",
              "      <td>0.425533</td>\n",
              "      <td>-0.849139</td>\n",
              "      <td>-0.951574</td>\n",
              "      <td>-0.999351</td>\n",
              "      <td>-0.999067</td>\n",
              "      <td>-0.999030</td>\n",
              "      <td>-0.999179</td>\n",
              "      <td>-0.997477</td>\n",
              "      <td>-0.999351</td>\n",
              "      <td>-0.999997</td>\n",
              "      <td>-0.998989</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-0.128205</td>\n",
              "      <td>0.485600</td>\n",
              "      <td>-0.681152</td>\n",
              "      <td>-0.921088</td>\n",
              "      <td>-0.999208</td>\n",
              "      <td>-0.999149</td>\n",
              "      <td>-0.999005</td>\n",
              "      <td>-0.999489</td>\n",
              "      <td>-0.995933</td>\n",
              "      <td>-0.999208</td>\n",
              "      <td>-0.999998</td>\n",
              "      <td>-0.998522</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-0.841270</td>\n",
              "      <td>0.550534</td>\n",
              "      <td>-0.751351</td>\n",
              "      <td>-0.938385</td>\n",
              "      <td>-0.260217</td>\n",
              "      <td>0.784700</td>\n",
              "      <td>-0.203905</td>\n",
              "      <td>0.073069</td>\n",
              "      <td>0.344459</td>\n",
              "      <td>-0.790490</td>\n",
              "      <td>-0.113880</td>\n",
              "      <td>11</td>\n",
              "      <td>LAYING</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 563 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      tBodyAcc-mean()-X  tBodyAcc-mean()-Y  ...  subject            Activity\n",
              "5935           0.274728          -0.014904  ...       27             SITTING\n",
              "1076           0.275425          -0.018261  ...        6              LAYING\n",
              "1445           0.318558          -0.072246  ...        7  WALKING_DOWNSTAIRS\n",
              "3817           0.276492          -0.007673  ...       19            STANDING\n",
              "1961           0.279312          -0.016641  ...       11              LAYING\n",
              "\n",
              "[5 rows x 563 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLb8kDhD7gEg",
        "outputId": "4a7a6d04-54fd-4083-bcda-d4095fc4a4b5"
      },
      "source": [
        "print(\"Shape of training set:\", train.shape)\n",
        "print(\"Shape of testing set:\",test.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of training set: (7352, 563)\n",
            "Shape of testing set: (2947, 563)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpBdS73C7gEg",
        "outputId": "6495e5eb-a887-4756-dbd5-6523f87738c5"
      },
      "source": [
        "# Seperate subject information\n",
        "subject_training_data = train['subject']\n",
        "subject_testing_data = test['subject']\n",
        "\n",
        "# Seperate labels\n",
        "training_labels = train['Activity']\n",
        "testing_labels = test['Activity']\n",
        "\n",
        "# Drop labels and subject info from data\n",
        "train = train.drop(['subject', 'Activity'], axis=1)\n",
        "test = test.drop(['subject', 'Activity'], axis=1)\n",
        "\n",
        "print(\"Training data consists of {} instances of data with {} total features\".format(train.shape[0], train.shape[1]))\n",
        "print(\"Training data includes value counts of\\n\", training_labels.value_counts())\n",
        "print(\"Testing data consists of {} instances of data\".format(test.shape[0]))\n",
        "print(\"Testing data includes value counts of\\n\", testing_labels.value_counts())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data consists of 7352 instances of data with 561 total features\n",
            "Training data includes value counts of\n",
            " LAYING                1407\n",
            "STANDING              1374\n",
            "SITTING               1286\n",
            "WALKING               1226\n",
            "WALKING_UPSTAIRS      1073\n",
            "WALKING_DOWNSTAIRS     986\n",
            "Name: Activity, dtype: int64\n",
            "Testing data consists of 2947 instances of data\n",
            "Testing data includes value counts of\n",
            " LAYING                537\n",
            "STANDING              532\n",
            "WALKING               496\n",
            "SITTING               491\n",
            "WALKING_UPSTAIRS      471\n",
            "WALKING_DOWNSTAIRS    420\n",
            "Name: Activity, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ij2w7IJm7gEg"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "le = le.fit([\"WALKING\", \"WALKING_UPSTAIRS\", \"WALKING_DOWNSTAIRS\", \"SITTING\", \"STANDING\", \"LAYING\"])\n",
        "enc_training_labels = le.transform(training_labels)\n",
        "enc_testing_labels = le.transform(testing_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OykHp3Y17gEg",
        "outputId": "09ce3668-76a3-46bb-daf7-88e74924e906"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier \n",
        "\n",
        "dt = DecisionTreeClassifier()\n",
        "kn = KNeighborsClassifier()\n",
        "lr = LogisticRegression(random_state=0 )\n",
        "\n",
        "def evaluateclf(clf):\n",
        "    scores = cross_val_score(clf, train, enc_training_labels)\n",
        "    avg = scores.mean()\n",
        "    return \"performances: {}, \\nAverage: {}\".format(scores, avg)\n",
        "\n",
        "print(\"Decision Tree {}\".format(evaluateclf(dt)))\n",
        "print(\"Logistic Regression {}\".format(evaluateclf(lr)))                        \n",
        "print(\"K Neighbors {}\".format(evaluateclf(kn)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decision Tree performances: [0.93201903 0.93881713 0.9414966  0.93741497 0.94013605], \n",
            "Average: 0.9379767569842349\n",
            "Logistic Regression performances: [0.98436438 0.97484704 0.98911565 0.97959184 0.98435374], \n",
            "Average: 0.9824545290583944\n",
            "K Neighbors performances: [0.96193066 0.96532971 0.96530612 0.97278912 0.97142857], \n",
            "Average: 0.9673568353242045\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XWO-GYM7gEg"
      },
      "source": [
        "# !pip install tensorflow==2.2\n",
        "# !pip install keras\n",
        "from keras.utils.np_utils import to_categorical\n",
        "oh_training_labels = to_categorical(enc_training_labels)\n",
        "oh_testing_labels = to_categorical(enc_testing_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lH-UVbVA7gEg",
        "outputId": "f2e09dfd-a50e-48d7-d2ec-2f2fb3e262ab"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.optimizers import SGD\n",
        "from keras.regularizers import l2\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# Build a network for this classification task\n",
        "model = Sequential()\n",
        "model.add(Dense(96, input_dim = train.shape[1], activation = 'tanh'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(30, activation = 'tanh'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(24, activation = 'tanh'))\n",
        "model.add(Dense(6, activation = 'softmax'))\n",
        "\n",
        "sgd = SGD(lr = .13, momentum = .9, decay = 4e-3)\n",
        "model.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "history = model.fit(train.values, oh_training_labels, epochs = 120, batch_size = 50, verbose = 2,\n",
        "          validation_split = .15, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/120\n",
            "125/125 - 1s - loss: 1.0220 - accuracy: 0.5006 - val_loss: 0.6782 - val_accuracy: 0.6337\n",
            "Epoch 2/120\n",
            "125/125 - 0s - loss: 0.7746 - accuracy: 0.6284 - val_loss: 0.4827 - val_accuracy: 0.7534\n",
            "Epoch 3/120\n",
            "125/125 - 0s - loss: 0.6312 - accuracy: 0.6923 - val_loss: 0.4889 - val_accuracy: 0.7525\n",
            "Epoch 4/120\n",
            "125/125 - 0s - loss: 0.5259 - accuracy: 0.7384 - val_loss: 0.3938 - val_accuracy: 0.7752\n",
            "Epoch 5/120\n",
            "125/125 - 0s - loss: 0.4621 - accuracy: 0.7564 - val_loss: 0.4261 - val_accuracy: 0.7851\n",
            "Epoch 6/120\n",
            "125/125 - 0s - loss: 0.4744 - accuracy: 0.7505 - val_loss: 0.3318 - val_accuracy: 0.7915\n",
            "Epoch 7/120\n",
            "125/125 - 0s - loss: 0.4089 - accuracy: 0.7798 - val_loss: 0.3162 - val_accuracy: 0.7897\n",
            "Epoch 8/120\n",
            "125/125 - 0s - loss: 0.3941 - accuracy: 0.7870 - val_loss: 0.2943 - val_accuracy: 0.8005\n",
            "Epoch 9/120\n",
            "125/125 - 0s - loss: 0.3639 - accuracy: 0.7972 - val_loss: 0.2922 - val_accuracy: 0.8223\n",
            "Epoch 10/120\n",
            "125/125 - 0s - loss: 0.3555 - accuracy: 0.8051 - val_loss: 0.2917 - val_accuracy: 0.8214\n",
            "Epoch 11/120\n",
            "125/125 - 0s - loss: 0.3311 - accuracy: 0.8075 - val_loss: 0.3007 - val_accuracy: 0.8205\n",
            "Epoch 12/120\n",
            "125/125 - 0s - loss: 0.3415 - accuracy: 0.8056 - val_loss: 0.2743 - val_accuracy: 0.8368\n",
            "Epoch 13/120\n",
            "125/125 - 0s - loss: 0.3270 - accuracy: 0.8238 - val_loss: 0.2722 - val_accuracy: 0.8377\n",
            "Epoch 14/120\n",
            "125/125 - 0s - loss: 0.3165 - accuracy: 0.8294 - val_loss: 0.2601 - val_accuracy: 0.8731\n",
            "Epoch 15/120\n",
            "125/125 - 0s - loss: 0.2898 - accuracy: 0.8472 - val_loss: 0.2637 - val_accuracy: 0.8821\n",
            "Epoch 16/120\n",
            "125/125 - 0s - loss: 0.2976 - accuracy: 0.8401 - val_loss: 0.2392 - val_accuracy: 0.8885\n",
            "Epoch 17/120\n",
            "125/125 - 0s - loss: 0.2827 - accuracy: 0.8536 - val_loss: 0.2249 - val_accuracy: 0.8976\n",
            "Epoch 18/120\n",
            "125/125 - 0s - loss: 0.2755 - accuracy: 0.8629 - val_loss: 0.2332 - val_accuracy: 0.8966\n",
            "Epoch 19/120\n",
            "125/125 - 0s - loss: 0.2667 - accuracy: 0.8704 - val_loss: 0.2259 - val_accuracy: 0.8921\n",
            "Epoch 20/120\n",
            "125/125 - 0s - loss: 0.2562 - accuracy: 0.8758 - val_loss: 0.2128 - val_accuracy: 0.9148\n",
            "Epoch 21/120\n",
            "125/125 - 0s - loss: 0.2498 - accuracy: 0.8817 - val_loss: 0.2006 - val_accuracy: 0.9148\n",
            "Epoch 22/120\n",
            "125/125 - 0s - loss: 0.2367 - accuracy: 0.8925 - val_loss: 0.1946 - val_accuracy: 0.9202\n",
            "Epoch 23/120\n",
            "125/125 - 0s - loss: 0.2469 - accuracy: 0.8878 - val_loss: 0.1775 - val_accuracy: 0.9211\n",
            "Epoch 24/120\n",
            "125/125 - 0s - loss: 0.2404 - accuracy: 0.8897 - val_loss: 0.1886 - val_accuracy: 0.9193\n",
            "Epoch 25/120\n",
            "125/125 - 0s - loss: 0.2245 - accuracy: 0.8992 - val_loss: 0.1855 - val_accuracy: 0.9257\n",
            "Epoch 26/120\n",
            "125/125 - 0s - loss: 0.2266 - accuracy: 0.8976 - val_loss: 0.2022 - val_accuracy: 0.9248\n",
            "Epoch 27/120\n",
            "125/125 - 0s - loss: 0.2146 - accuracy: 0.9046 - val_loss: 0.1691 - val_accuracy: 0.9320\n",
            "Epoch 28/120\n",
            "125/125 - 0s - loss: 0.2145 - accuracy: 0.9128 - val_loss: 0.1677 - val_accuracy: 0.9329\n",
            "Epoch 29/120\n",
            "125/125 - 0s - loss: 0.2098 - accuracy: 0.9093 - val_loss: 0.1508 - val_accuracy: 0.9402\n",
            "Epoch 30/120\n",
            "125/125 - 0s - loss: 0.1923 - accuracy: 0.9209 - val_loss: 0.1781 - val_accuracy: 0.9293\n",
            "Epoch 31/120\n",
            "125/125 - 0s - loss: 0.2042 - accuracy: 0.9112 - val_loss: 0.1909 - val_accuracy: 0.9175\n",
            "Epoch 32/120\n",
            "125/125 - 0s - loss: 0.2102 - accuracy: 0.9088 - val_loss: 0.1450 - val_accuracy: 0.9456\n",
            "Epoch 33/120\n",
            "125/125 - 0s - loss: 0.1969 - accuracy: 0.9166 - val_loss: 0.2035 - val_accuracy: 0.9166\n",
            "Epoch 34/120\n",
            "125/125 - 0s - loss: 0.1855 - accuracy: 0.9217 - val_loss: 0.1425 - val_accuracy: 0.9483\n",
            "Epoch 35/120\n",
            "125/125 - 0s - loss: 0.1838 - accuracy: 0.9264 - val_loss: 0.1317 - val_accuracy: 0.9510\n",
            "Epoch 36/120\n",
            "125/125 - 0s - loss: 0.1772 - accuracy: 0.9285 - val_loss: 0.1314 - val_accuracy: 0.9529\n",
            "Epoch 37/120\n",
            "125/125 - 0s - loss: 0.1832 - accuracy: 0.9241 - val_loss: 0.1315 - val_accuracy: 0.9529\n",
            "Epoch 38/120\n",
            "125/125 - 0s - loss: 0.1754 - accuracy: 0.9253 - val_loss: 0.1331 - val_accuracy: 0.9510\n",
            "Epoch 39/120\n",
            "125/125 - 0s - loss: 0.1674 - accuracy: 0.9304 - val_loss: 0.1307 - val_accuracy: 0.9556\n",
            "Epoch 40/120\n",
            "125/125 - 0s - loss: 0.1691 - accuracy: 0.9345 - val_loss: 0.2316 - val_accuracy: 0.9175\n",
            "Epoch 41/120\n",
            "125/125 - 0s - loss: 0.1688 - accuracy: 0.9339 - val_loss: 0.1358 - val_accuracy: 0.9483\n",
            "Epoch 42/120\n",
            "125/125 - 0s - loss: 0.1672 - accuracy: 0.9328 - val_loss: 0.2281 - val_accuracy: 0.9075\n",
            "Epoch 43/120\n",
            "125/125 - 0s - loss: 0.1725 - accuracy: 0.9302 - val_loss: 0.1174 - val_accuracy: 0.9583\n",
            "Epoch 44/120\n",
            "125/125 - 0s - loss: 0.1641 - accuracy: 0.9360 - val_loss: 0.1222 - val_accuracy: 0.9556\n",
            "Epoch 45/120\n",
            "125/125 - 0s - loss: 0.1571 - accuracy: 0.9369 - val_loss: 0.1186 - val_accuracy: 0.9592\n",
            "Epoch 46/120\n",
            "125/125 - 0s - loss: 0.1535 - accuracy: 0.9398 - val_loss: 0.1540 - val_accuracy: 0.9492\n",
            "Epoch 47/120\n",
            "125/125 - 0s - loss: 0.1604 - accuracy: 0.9413 - val_loss: 0.1222 - val_accuracy: 0.9619\n",
            "Epoch 48/120\n",
            "125/125 - 0s - loss: 0.1473 - accuracy: 0.9464 - val_loss: 0.1379 - val_accuracy: 0.9510\n",
            "Epoch 49/120\n",
            "125/125 - 0s - loss: 0.1450 - accuracy: 0.9438 - val_loss: 0.1180 - val_accuracy: 0.9583\n",
            "Epoch 50/120\n",
            "125/125 - 0s - loss: 0.1403 - accuracy: 0.9469 - val_loss: 0.1163 - val_accuracy: 0.9592\n",
            "Epoch 51/120\n",
            "125/125 - 0s - loss: 0.1435 - accuracy: 0.9434 - val_loss: 0.1241 - val_accuracy: 0.9538\n",
            "Epoch 52/120\n",
            "125/125 - 0s - loss: 0.1468 - accuracy: 0.9454 - val_loss: 0.1059 - val_accuracy: 0.9655\n",
            "Epoch 53/120\n",
            "125/125 - 0s - loss: 0.1481 - accuracy: 0.9427 - val_loss: 0.1151 - val_accuracy: 0.9637\n",
            "Epoch 54/120\n",
            "125/125 - 0s - loss: 0.1338 - accuracy: 0.9514 - val_loss: 0.1123 - val_accuracy: 0.9601\n",
            "Epoch 55/120\n",
            "125/125 - 0s - loss: 0.1342 - accuracy: 0.9531 - val_loss: 0.1137 - val_accuracy: 0.9592\n",
            "Epoch 56/120\n",
            "125/125 - 0s - loss: 0.1310 - accuracy: 0.9517 - val_loss: 0.1236 - val_accuracy: 0.9556\n",
            "Epoch 57/120\n",
            "125/125 - 0s - loss: 0.1391 - accuracy: 0.9482 - val_loss: 0.1096 - val_accuracy: 0.9574\n",
            "Epoch 58/120\n",
            "125/125 - 0s - loss: 0.1352 - accuracy: 0.9486 - val_loss: 0.1142 - val_accuracy: 0.9637\n",
            "Epoch 59/120\n",
            "125/125 - 0s - loss: 0.1417 - accuracy: 0.9486 - val_loss: 0.0996 - val_accuracy: 0.9655\n",
            "Epoch 60/120\n",
            "125/125 - 0s - loss: 0.1227 - accuracy: 0.9554 - val_loss: 0.1003 - val_accuracy: 0.9655\n",
            "Epoch 61/120\n",
            "125/125 - 0s - loss: 0.1167 - accuracy: 0.9546 - val_loss: 0.1028 - val_accuracy: 0.9628\n",
            "Epoch 62/120\n",
            "125/125 - 0s - loss: 0.1245 - accuracy: 0.9544 - val_loss: 0.1014 - val_accuracy: 0.9646\n",
            "Epoch 63/120\n",
            "125/125 - 0s - loss: 0.1259 - accuracy: 0.9531 - val_loss: 0.1120 - val_accuracy: 0.9592\n",
            "Epoch 64/120\n",
            "125/125 - 0s - loss: 0.1327 - accuracy: 0.9509 - val_loss: 0.0958 - val_accuracy: 0.9683\n",
            "Epoch 65/120\n",
            "125/125 - 0s - loss: 0.1213 - accuracy: 0.9536 - val_loss: 0.0963 - val_accuracy: 0.9692\n",
            "Epoch 66/120\n",
            "125/125 - 0s - loss: 0.1134 - accuracy: 0.9586 - val_loss: 0.0986 - val_accuracy: 0.9655\n",
            "Epoch 67/120\n",
            "125/125 - 0s - loss: 0.1124 - accuracy: 0.9586 - val_loss: 0.0925 - val_accuracy: 0.9701\n",
            "Epoch 68/120\n",
            "125/125 - 0s - loss: 0.1201 - accuracy: 0.9560 - val_loss: 0.1130 - val_accuracy: 0.9610\n",
            "Epoch 69/120\n",
            "125/125 - 0s - loss: 0.1216 - accuracy: 0.9546 - val_loss: 0.0926 - val_accuracy: 0.9737\n",
            "Epoch 70/120\n",
            "125/125 - 0s - loss: 0.1235 - accuracy: 0.9520 - val_loss: 0.0943 - val_accuracy: 0.9674\n",
            "Epoch 71/120\n",
            "125/125 - 0s - loss: 0.1169 - accuracy: 0.9562 - val_loss: 0.0972 - val_accuracy: 0.9646\n",
            "Epoch 72/120\n",
            "125/125 - 0s - loss: 0.1112 - accuracy: 0.9602 - val_loss: 0.0921 - val_accuracy: 0.9710\n",
            "Epoch 73/120\n",
            "125/125 - 0s - loss: 0.1090 - accuracy: 0.9606 - val_loss: 0.0862 - val_accuracy: 0.9710\n",
            "Epoch 74/120\n",
            "125/125 - 0s - loss: 0.1150 - accuracy: 0.9582 - val_loss: 0.0937 - val_accuracy: 0.9746\n",
            "Epoch 75/120\n",
            "125/125 - 0s - loss: 0.1036 - accuracy: 0.9632 - val_loss: 0.0941 - val_accuracy: 0.9728\n",
            "Epoch 76/120\n",
            "125/125 - 0s - loss: 0.1051 - accuracy: 0.9632 - val_loss: 0.0916 - val_accuracy: 0.9692\n",
            "Epoch 77/120\n",
            "125/125 - 0s - loss: 0.1060 - accuracy: 0.9621 - val_loss: 0.1252 - val_accuracy: 0.9637\n",
            "Epoch 78/120\n",
            "125/125 - 0s - loss: 0.1129 - accuracy: 0.9610 - val_loss: 0.0972 - val_accuracy: 0.9646\n",
            "Epoch 79/120\n",
            "125/125 - 0s - loss: 0.1032 - accuracy: 0.9642 - val_loss: 0.1168 - val_accuracy: 0.9574\n",
            "Epoch 80/120\n",
            "125/125 - 0s - loss: 0.1065 - accuracy: 0.9622 - val_loss: 0.0982 - val_accuracy: 0.9728\n",
            "Epoch 81/120\n",
            "125/125 - 0s - loss: 0.1001 - accuracy: 0.9648 - val_loss: 0.0898 - val_accuracy: 0.9746\n",
            "Epoch 82/120\n",
            "125/125 - 0s - loss: 0.1030 - accuracy: 0.9648 - val_loss: 0.0878 - val_accuracy: 0.9755\n",
            "Epoch 83/120\n",
            "125/125 - 0s - loss: 0.0972 - accuracy: 0.9661 - val_loss: 0.1194 - val_accuracy: 0.9601\n",
            "Epoch 84/120\n",
            "125/125 - 0s - loss: 0.1022 - accuracy: 0.9619 - val_loss: 0.1350 - val_accuracy: 0.9529\n",
            "Epoch 85/120\n",
            "125/125 - 0s - loss: 0.0987 - accuracy: 0.9664 - val_loss: 0.1183 - val_accuracy: 0.9592\n",
            "Epoch 86/120\n",
            "125/125 - 0s - loss: 0.1050 - accuracy: 0.9626 - val_loss: 0.0911 - val_accuracy: 0.9719\n",
            "Epoch 87/120\n",
            "125/125 - 0s - loss: 0.1022 - accuracy: 0.9626 - val_loss: 0.0905 - val_accuracy: 0.9719\n",
            "Epoch 88/120\n",
            "125/125 - 0s - loss: 0.0970 - accuracy: 0.9635 - val_loss: 0.0986 - val_accuracy: 0.9737\n",
            "Epoch 89/120\n",
            "125/125 - 0s - loss: 0.0939 - accuracy: 0.9666 - val_loss: 0.1033 - val_accuracy: 0.9665\n",
            "Epoch 90/120\n",
            "125/125 - 0s - loss: 0.0991 - accuracy: 0.9630 - val_loss: 0.0896 - val_accuracy: 0.9728\n",
            "Epoch 91/120\n",
            "125/125 - 0s - loss: 0.0989 - accuracy: 0.9656 - val_loss: 0.0867 - val_accuracy: 0.9737\n",
            "Epoch 92/120\n",
            "125/125 - 0s - loss: 0.1003 - accuracy: 0.9659 - val_loss: 0.0869 - val_accuracy: 0.9737\n",
            "Epoch 93/120\n",
            "125/125 - 0s - loss: 0.0878 - accuracy: 0.9666 - val_loss: 0.0907 - val_accuracy: 0.9764\n",
            "Epoch 94/120\n",
            "125/125 - 0s - loss: 0.0848 - accuracy: 0.9709 - val_loss: 0.0937 - val_accuracy: 0.9719\n",
            "Epoch 95/120\n",
            "125/125 - 0s - loss: 0.0908 - accuracy: 0.9675 - val_loss: 0.0888 - val_accuracy: 0.9746\n",
            "Epoch 96/120\n",
            "125/125 - 0s - loss: 0.0935 - accuracy: 0.9661 - val_loss: 0.1578 - val_accuracy: 0.9474\n",
            "Epoch 97/120\n",
            "125/125 - 0s - loss: 0.1051 - accuracy: 0.9627 - val_loss: 0.1014 - val_accuracy: 0.9665\n",
            "Epoch 98/120\n",
            "125/125 - 0s - loss: 0.1047 - accuracy: 0.9630 - val_loss: 0.0907 - val_accuracy: 0.9728\n",
            "Epoch 99/120\n",
            "125/125 - 0s - loss: 0.0970 - accuracy: 0.9645 - val_loss: 0.0917 - val_accuracy: 0.9764\n",
            "Epoch 100/120\n",
            "125/125 - 0s - loss: 0.0978 - accuracy: 0.9664 - val_loss: 0.0893 - val_accuracy: 0.9764\n",
            "Epoch 101/120\n",
            "125/125 - 0s - loss: 0.0956 - accuracy: 0.9634 - val_loss: 0.0916 - val_accuracy: 0.9701\n",
            "Epoch 102/120\n",
            "125/125 - 0s - loss: 0.0986 - accuracy: 0.9638 - val_loss: 0.0891 - val_accuracy: 0.9728\n",
            "Epoch 103/120\n",
            "125/125 - 0s - loss: 0.0951 - accuracy: 0.9669 - val_loss: 0.0947 - val_accuracy: 0.9737\n",
            "Epoch 104/120\n",
            "125/125 - 0s - loss: 0.0917 - accuracy: 0.9672 - val_loss: 0.0876 - val_accuracy: 0.9773\n",
            "Epoch 105/120\n",
            "125/125 - 0s - loss: 0.0882 - accuracy: 0.9702 - val_loss: 0.0860 - val_accuracy: 0.9728\n",
            "Epoch 106/120\n",
            "125/125 - 0s - loss: 0.0911 - accuracy: 0.9698 - val_loss: 0.0879 - val_accuracy: 0.9719\n",
            "Epoch 107/120\n",
            "125/125 - 0s - loss: 0.0867 - accuracy: 0.9688 - val_loss: 0.0849 - val_accuracy: 0.9728\n",
            "Epoch 108/120\n",
            "125/125 - 0s - loss: 0.0896 - accuracy: 0.9698 - val_loss: 0.0910 - val_accuracy: 0.9719\n",
            "Epoch 109/120\n",
            "125/125 - 0s - loss: 0.0901 - accuracy: 0.9656 - val_loss: 0.0883 - val_accuracy: 0.9773\n",
            "Epoch 110/120\n",
            "125/125 - 0s - loss: 0.0862 - accuracy: 0.9706 - val_loss: 0.0879 - val_accuracy: 0.9737\n",
            "Epoch 111/120\n",
            "125/125 - 0s - loss: 0.0831 - accuracy: 0.9714 - val_loss: 0.0866 - val_accuracy: 0.9710\n",
            "Epoch 112/120\n",
            "125/125 - 0s - loss: 0.0868 - accuracy: 0.9682 - val_loss: 0.1206 - val_accuracy: 0.9592\n",
            "Epoch 113/120\n",
            "125/125 - 0s - loss: 0.0827 - accuracy: 0.9718 - val_loss: 0.0870 - val_accuracy: 0.9728\n",
            "Epoch 114/120\n",
            "125/125 - 0s - loss: 0.0815 - accuracy: 0.9728 - val_loss: 0.0847 - val_accuracy: 0.9746\n",
            "Epoch 115/120\n",
            "125/125 - 0s - loss: 0.0840 - accuracy: 0.9699 - val_loss: 0.0848 - val_accuracy: 0.9728\n",
            "Epoch 116/120\n",
            "125/125 - 0s - loss: 0.0891 - accuracy: 0.9678 - val_loss: 0.0951 - val_accuracy: 0.9710\n",
            "Epoch 117/120\n",
            "125/125 - 0s - loss: 0.1000 - accuracy: 0.9646 - val_loss: 0.0919 - val_accuracy: 0.9728\n",
            "Epoch 118/120\n",
            "125/125 - 0s - loss: 0.0822 - accuracy: 0.9704 - val_loss: 0.0844 - val_accuracy: 0.9746\n",
            "Epoch 119/120\n",
            "125/125 - 0s - loss: 0.0808 - accuracy: 0.9709 - val_loss: 0.0860 - val_accuracy: 0.9737\n",
            "Epoch 120/120\n",
            "125/125 - 0s - loss: 0.0786 - accuracy: 0.9749 - val_loss: 0.0849 - val_accuracy: 0.9755\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TX899FcT7gEg",
        "outputId": "0716f2d6-2a41-44a6-bb0a-e4dbbf08ba85"
      },
      "source": [
        "nn_test_score = model.evaluate(test.values, oh_testing_labels, verbose=2)\n",
        "print(\"Neural Network accuracy of {} on the test set\".format(nn_test_score[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "93/93 - 0s - loss: 0.1798 - accuracy: 0.9505\n",
            "Neural Network accuracy of 0.9504581093788147 on the test set\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "zSpO-mFC7gEh",
        "outputId": "861c1658-a849-4a4f-9bc1-447beba2ed8d"
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('categorical cross entropy loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['training loss', 'validation loss'], loc = 'upper right' )\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGmOJImp8PnX",
        "outputId": "dca50534-f355-4a35-9836-cb0b81423337"
      },
      "source": [
        "# Using relu activation function\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.optimizers import SGD\n",
        "from keras.regularizers import l2\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# Build a network for this classification task\n",
        "model = Sequential()\n",
        "model.add(Dense(96, input_dim = train.shape[1], activation = 'tanh'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(30, activation = 'relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(24, activation = 'relu'))\n",
        "model.add(Dense(6, activation = 'softmax'))\n",
        "\n",
        "sgd = SGD(lr = .13, momentum = .9, decay = 4e-3)\n",
        "model.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "history = model.fit(train.values, oh_training_labels, epochs = 120, batch_size = 50, verbose = 2,\n",
        "          validation_split = .15, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/120\n",
            "125/125 - 1s - loss: 1.2565 - accuracy: 0.3471 - val_loss: 1.0708 - val_accuracy: 0.4180\n",
            "Epoch 2/120\n",
            "125/125 - 0s - loss: 1.0830 - accuracy: 0.4076 - val_loss: 0.9138 - val_accuracy: 0.4651\n",
            "Epoch 3/120\n",
            "125/125 - 0s - loss: 0.8671 - accuracy: 0.5393 - val_loss: 0.5921 - val_accuracy: 0.6491\n",
            "Epoch 4/120\n",
            "125/125 - 0s - loss: 0.6710 - accuracy: 0.6327 - val_loss: 0.5409 - val_accuracy: 0.6600\n",
            "Epoch 5/120\n",
            "125/125 - 0s - loss: 0.6496 - accuracy: 0.6350 - val_loss: 0.5423 - val_accuracy: 0.6591\n",
            "Epoch 6/120\n",
            "125/125 - 0s - loss: 0.6591 - accuracy: 0.6358 - val_loss: 0.5171 - val_accuracy: 0.6682\n",
            "Epoch 7/120\n",
            "125/125 - 0s - loss: 0.5947 - accuracy: 0.6548 - val_loss: 0.5034 - val_accuracy: 0.6854\n",
            "Epoch 8/120\n",
            "125/125 - 0s - loss: 0.5647 - accuracy: 0.6596 - val_loss: 0.4947 - val_accuracy: 0.6691\n",
            "Epoch 9/120\n",
            "125/125 - 0s - loss: 0.5686 - accuracy: 0.6604 - val_loss: 0.4936 - val_accuracy: 0.6700\n",
            "Epoch 10/120\n",
            "125/125 - 0s - loss: 0.5450 - accuracy: 0.6812 - val_loss: 0.5006 - val_accuracy: 0.6908\n",
            "Epoch 11/120\n",
            "125/125 - 0s - loss: 0.5086 - accuracy: 0.7025 - val_loss: 0.4272 - val_accuracy: 0.7788\n",
            "Epoch 12/120\n",
            "125/125 - 0s - loss: 0.4728 - accuracy: 0.7260 - val_loss: 0.4213 - val_accuracy: 0.7534\n",
            "Epoch 13/120\n",
            "125/125 - 0s - loss: 0.4342 - accuracy: 0.7508 - val_loss: 0.3464 - val_accuracy: 0.8005\n",
            "Epoch 14/120\n",
            "125/125 - 0s - loss: 0.4366 - accuracy: 0.7561 - val_loss: 0.3044 - val_accuracy: 0.8314\n",
            "Epoch 15/120\n",
            "125/125 - 0s - loss: 0.4073 - accuracy: 0.7539 - val_loss: 0.3056 - val_accuracy: 0.8178\n",
            "Epoch 16/120\n",
            "125/125 - 0s - loss: 0.3996 - accuracy: 0.7729 - val_loss: 0.2755 - val_accuracy: 0.8504\n",
            "Epoch 17/120\n",
            "125/125 - 0s - loss: 0.3850 - accuracy: 0.7824 - val_loss: 0.2875 - val_accuracy: 0.8341\n",
            "Epoch 18/120\n",
            "125/125 - 0s - loss: 0.3674 - accuracy: 0.7891 - val_loss: 0.2782 - val_accuracy: 0.8060\n",
            "Epoch 19/120\n",
            "125/125 - 0s - loss: 0.3521 - accuracy: 0.7908 - val_loss: 0.2691 - val_accuracy: 0.8214\n",
            "Epoch 20/120\n",
            "125/125 - 0s - loss: 0.3539 - accuracy: 0.8008 - val_loss: 0.2677 - val_accuracy: 0.8087\n",
            "Epoch 21/120\n",
            "125/125 - 0s - loss: 0.3431 - accuracy: 0.7958 - val_loss: 0.2673 - val_accuracy: 0.8087\n",
            "Epoch 22/120\n",
            "125/125 - 0s - loss: 0.3433 - accuracy: 0.7976 - val_loss: 0.2588 - val_accuracy: 0.8704\n",
            "Epoch 23/120\n",
            "125/125 - 0s - loss: 0.3283 - accuracy: 0.8105 - val_loss: 0.2578 - val_accuracy: 0.8867\n",
            "Epoch 24/120\n",
            "125/125 - 0s - loss: 0.3282 - accuracy: 0.8137 - val_loss: 0.2661 - val_accuracy: 0.8069\n",
            "Epoch 25/120\n",
            "125/125 - 0s - loss: 0.3219 - accuracy: 0.8147 - val_loss: 0.2527 - val_accuracy: 0.8694\n",
            "Epoch 26/120\n",
            "125/125 - 0s - loss: 0.3154 - accuracy: 0.8190 - val_loss: 0.2501 - val_accuracy: 0.9130\n",
            "Epoch 27/120\n",
            "125/125 - 0s - loss: 0.3235 - accuracy: 0.8273 - val_loss: 0.2561 - val_accuracy: 0.8794\n",
            "Epoch 28/120\n",
            "125/125 - 0s - loss: 0.3029 - accuracy: 0.8328 - val_loss: 0.2371 - val_accuracy: 0.8876\n",
            "Epoch 29/120\n",
            "125/125 - 0s - loss: 0.3106 - accuracy: 0.8275 - val_loss: 0.2472 - val_accuracy: 0.9112\n",
            "Epoch 30/120\n",
            "125/125 - 0s - loss: 0.2962 - accuracy: 0.8453 - val_loss: 0.2244 - val_accuracy: 0.9293\n",
            "Epoch 31/120\n",
            "125/125 - 0s - loss: 0.2841 - accuracy: 0.8525 - val_loss: 0.2112 - val_accuracy: 0.9166\n",
            "Epoch 32/120\n",
            "125/125 - 0s - loss: 0.2962 - accuracy: 0.8510 - val_loss: 0.2047 - val_accuracy: 0.9193\n",
            "Epoch 33/120\n",
            "125/125 - 0s - loss: 0.2668 - accuracy: 0.8649 - val_loss: 0.1780 - val_accuracy: 0.9347\n",
            "Epoch 34/120\n",
            "125/125 - 0s - loss: 0.2599 - accuracy: 0.8734 - val_loss: 0.1721 - val_accuracy: 0.9402\n",
            "Epoch 35/120\n",
            "125/125 - 0s - loss: 0.2496 - accuracy: 0.8731 - val_loss: 0.1846 - val_accuracy: 0.9393\n",
            "Epoch 36/120\n",
            "125/125 - 0s - loss: 0.2423 - accuracy: 0.8821 - val_loss: 0.1609 - val_accuracy: 0.9429\n",
            "Epoch 37/120\n",
            "125/125 - 0s - loss: 0.2328 - accuracy: 0.8889 - val_loss: 0.1615 - val_accuracy: 0.9447\n",
            "Epoch 38/120\n",
            "125/125 - 0s - loss: 0.2390 - accuracy: 0.8838 - val_loss: 0.1523 - val_accuracy: 0.9429\n",
            "Epoch 39/120\n",
            "125/125 - 0s - loss: 0.2226 - accuracy: 0.8934 - val_loss: 0.1719 - val_accuracy: 0.9284\n",
            "Epoch 40/120\n",
            "125/125 - 0s - loss: 0.2211 - accuracy: 0.8920 - val_loss: 0.1384 - val_accuracy: 0.9447\n",
            "Epoch 41/120\n",
            "125/125 - 0s - loss: 0.2104 - accuracy: 0.9032 - val_loss: 0.1271 - val_accuracy: 0.9547\n",
            "Epoch 42/120\n",
            "125/125 - 0s - loss: 0.2219 - accuracy: 0.8958 - val_loss: 0.1370 - val_accuracy: 0.9556\n",
            "Epoch 43/120\n",
            "125/125 - 0s - loss: 0.2186 - accuracy: 0.8990 - val_loss: 0.1321 - val_accuracy: 0.9601\n",
            "Epoch 44/120\n",
            "125/125 - 0s - loss: 0.2051 - accuracy: 0.9065 - val_loss: 0.1270 - val_accuracy: 0.9501\n",
            "Epoch 45/120\n",
            "125/125 - 0s - loss: 0.2059 - accuracy: 0.9097 - val_loss: 0.1365 - val_accuracy: 0.9420\n",
            "Epoch 46/120\n",
            "125/125 - 0s - loss: 0.1939 - accuracy: 0.9094 - val_loss: 0.1342 - val_accuracy: 0.9519\n",
            "Epoch 47/120\n",
            "125/125 - 0s - loss: 0.2026 - accuracy: 0.9094 - val_loss: 0.1056 - val_accuracy: 0.9646\n",
            "Epoch 48/120\n",
            "125/125 - 0s - loss: 0.1898 - accuracy: 0.9163 - val_loss: 0.1138 - val_accuracy: 0.9565\n",
            "Epoch 49/120\n",
            "125/125 - 0s - loss: 0.1866 - accuracy: 0.9195 - val_loss: 0.1079 - val_accuracy: 0.9665\n",
            "Epoch 50/120\n",
            "125/125 - 0s - loss: 0.1923 - accuracy: 0.9150 - val_loss: 0.1078 - val_accuracy: 0.9646\n",
            "Epoch 51/120\n",
            "125/125 - 0s - loss: 0.1663 - accuracy: 0.9225 - val_loss: 0.0998 - val_accuracy: 0.9637\n",
            "Epoch 52/120\n",
            "125/125 - 0s - loss: 0.1698 - accuracy: 0.9302 - val_loss: 0.1060 - val_accuracy: 0.9592\n",
            "Epoch 53/120\n",
            "125/125 - 0s - loss: 0.1784 - accuracy: 0.9198 - val_loss: 0.1216 - val_accuracy: 0.9547\n",
            "Epoch 54/120\n",
            "125/125 - 0s - loss: 0.1878 - accuracy: 0.9161 - val_loss: 0.1196 - val_accuracy: 0.9655\n",
            "Epoch 55/120\n",
            "125/125 - 0s - loss: 0.1725 - accuracy: 0.9241 - val_loss: 0.0872 - val_accuracy: 0.9737\n",
            "Epoch 56/120\n",
            "125/125 - 0s - loss: 0.1716 - accuracy: 0.9269 - val_loss: 0.0878 - val_accuracy: 0.9674\n",
            "Epoch 57/120\n",
            "125/125 - 0s - loss: 0.1647 - accuracy: 0.9310 - val_loss: 0.0850 - val_accuracy: 0.9737\n",
            "Epoch 58/120\n",
            "125/125 - 0s - loss: 0.1767 - accuracy: 0.9233 - val_loss: 0.0924 - val_accuracy: 0.9692\n",
            "Epoch 59/120\n",
            "125/125 - 0s - loss: 0.1633 - accuracy: 0.9305 - val_loss: 0.0815 - val_accuracy: 0.9755\n",
            "Epoch 60/120\n",
            "125/125 - 0s - loss: 0.1613 - accuracy: 0.9291 - val_loss: 0.0959 - val_accuracy: 0.9692\n",
            "Epoch 61/120\n",
            "125/125 - 0s - loss: 0.1516 - accuracy: 0.9323 - val_loss: 0.0889 - val_accuracy: 0.9737\n",
            "Epoch 62/120\n",
            "125/125 - 0s - loss: 0.1442 - accuracy: 0.9397 - val_loss: 0.0933 - val_accuracy: 0.9728\n",
            "Epoch 63/120\n",
            "125/125 - 0s - loss: 0.1537 - accuracy: 0.9387 - val_loss: 0.0994 - val_accuracy: 0.9674\n",
            "Epoch 64/120\n",
            "125/125 - 0s - loss: 0.1575 - accuracy: 0.9360 - val_loss: 0.0749 - val_accuracy: 0.9755\n",
            "Epoch 65/120\n",
            "125/125 - 0s - loss: 0.1538 - accuracy: 0.9339 - val_loss: 0.0890 - val_accuracy: 0.9719\n",
            "Epoch 66/120\n",
            "125/125 - 0s - loss: 0.1599 - accuracy: 0.9352 - val_loss: 0.0853 - val_accuracy: 0.9737\n",
            "Epoch 67/120\n",
            "125/125 - 0s - loss: 0.1498 - accuracy: 0.9405 - val_loss: 0.0827 - val_accuracy: 0.9737\n",
            "Epoch 68/120\n",
            "125/125 - 0s - loss: 0.1567 - accuracy: 0.9360 - val_loss: 0.0815 - val_accuracy: 0.9728\n",
            "Epoch 69/120\n",
            "125/125 - 0s - loss: 0.1563 - accuracy: 0.9336 - val_loss: 0.0916 - val_accuracy: 0.9701\n",
            "Epoch 70/120\n",
            "125/125 - 0s - loss: 0.1424 - accuracy: 0.9389 - val_loss: 0.0780 - val_accuracy: 0.9746\n",
            "Epoch 71/120\n",
            "125/125 - 0s - loss: 0.1473 - accuracy: 0.9416 - val_loss: 0.0821 - val_accuracy: 0.9701\n",
            "Epoch 72/120\n",
            "125/125 - 0s - loss: 0.1413 - accuracy: 0.9440 - val_loss: 0.1180 - val_accuracy: 0.9529\n",
            "Epoch 73/120\n",
            "125/125 - 0s - loss: 0.1357 - accuracy: 0.9434 - val_loss: 0.0711 - val_accuracy: 0.9755\n",
            "Epoch 74/120\n",
            "125/125 - 0s - loss: 0.1409 - accuracy: 0.9427 - val_loss: 0.0718 - val_accuracy: 0.9755\n",
            "Epoch 75/120\n",
            "125/125 - 0s - loss: 0.1336 - accuracy: 0.9450 - val_loss: 0.0843 - val_accuracy: 0.9728\n",
            "Epoch 76/120\n",
            "125/125 - 0s - loss: 0.1297 - accuracy: 0.9470 - val_loss: 0.0732 - val_accuracy: 0.9773\n",
            "Epoch 77/120\n",
            "125/125 - 0s - loss: 0.1343 - accuracy: 0.9467 - val_loss: 0.0800 - val_accuracy: 0.9719\n",
            "Epoch 78/120\n",
            "125/125 - 0s - loss: 0.1346 - accuracy: 0.9459 - val_loss: 0.0826 - val_accuracy: 0.9755\n",
            "Epoch 79/120\n",
            "125/125 - 0s - loss: 0.1415 - accuracy: 0.9421 - val_loss: 0.0759 - val_accuracy: 0.9773\n",
            "Epoch 80/120\n",
            "125/125 - 0s - loss: 0.1427 - accuracy: 0.9464 - val_loss: 0.0878 - val_accuracy: 0.9692\n",
            "Epoch 81/120\n",
            "125/125 - 0s - loss: 0.1310 - accuracy: 0.9480 - val_loss: 0.0819 - val_accuracy: 0.9692\n",
            "Epoch 82/120\n",
            "125/125 - 0s - loss: 0.1256 - accuracy: 0.9477 - val_loss: 0.0835 - val_accuracy: 0.9710\n",
            "Epoch 83/120\n",
            "125/125 - 0s - loss: 0.1282 - accuracy: 0.9518 - val_loss: 0.0719 - val_accuracy: 0.9728\n",
            "Epoch 84/120\n",
            "125/125 - 0s - loss: 0.1322 - accuracy: 0.9458 - val_loss: 0.0874 - val_accuracy: 0.9692\n",
            "Epoch 85/120\n",
            "125/125 - 0s - loss: 0.1392 - accuracy: 0.9427 - val_loss: 0.0682 - val_accuracy: 0.9773\n",
            "Epoch 86/120\n",
            "125/125 - 0s - loss: 0.1312 - accuracy: 0.9480 - val_loss: 0.0651 - val_accuracy: 0.9764\n",
            "Epoch 87/120\n",
            "125/125 - 0s - loss: 0.1318 - accuracy: 0.9493 - val_loss: 0.0785 - val_accuracy: 0.9701\n",
            "Epoch 88/120\n",
            "125/125 - 0s - loss: 0.1217 - accuracy: 0.9509 - val_loss: 0.0689 - val_accuracy: 0.9755\n",
            "Epoch 89/120\n",
            "125/125 - 0s - loss: 0.1151 - accuracy: 0.9560 - val_loss: 0.0847 - val_accuracy: 0.9692\n",
            "Epoch 90/120\n",
            "125/125 - 0s - loss: 0.1211 - accuracy: 0.9518 - val_loss: 0.0722 - val_accuracy: 0.9782\n",
            "Epoch 91/120\n",
            "125/125 - 0s - loss: 0.1251 - accuracy: 0.9491 - val_loss: 0.0734 - val_accuracy: 0.9746\n",
            "Epoch 92/120\n",
            "125/125 - 0s - loss: 0.1188 - accuracy: 0.9550 - val_loss: 0.0832 - val_accuracy: 0.9737\n",
            "Epoch 93/120\n",
            "125/125 - 0s - loss: 0.1311 - accuracy: 0.9490 - val_loss: 0.0814 - val_accuracy: 0.9728\n",
            "Epoch 94/120\n",
            "125/125 - 0s - loss: 0.1195 - accuracy: 0.9536 - val_loss: 0.0835 - val_accuracy: 0.9755\n",
            "Epoch 95/120\n",
            "125/125 - 0s - loss: 0.1288 - accuracy: 0.9531 - val_loss: 0.0741 - val_accuracy: 0.9782\n",
            "Epoch 96/120\n",
            "125/125 - 0s - loss: 0.1184 - accuracy: 0.9550 - val_loss: 0.0827 - val_accuracy: 0.9692\n",
            "Epoch 97/120\n",
            "125/125 - 0s - loss: 0.1199 - accuracy: 0.9566 - val_loss: 0.1211 - val_accuracy: 0.9538\n",
            "Epoch 98/120\n",
            "125/125 - 0s - loss: 0.1154 - accuracy: 0.9550 - val_loss: 0.0746 - val_accuracy: 0.9719\n",
            "Epoch 99/120\n",
            "125/125 - 0s - loss: 0.1099 - accuracy: 0.9590 - val_loss: 0.0631 - val_accuracy: 0.9782\n",
            "Epoch 100/120\n",
            "125/125 - 0s - loss: 0.1193 - accuracy: 0.9546 - val_loss: 0.0657 - val_accuracy: 0.9764\n",
            "Epoch 101/120\n",
            "125/125 - 0s - loss: 0.1239 - accuracy: 0.9522 - val_loss: 0.0712 - val_accuracy: 0.9764\n",
            "Epoch 102/120\n",
            "125/125 - 0s - loss: 0.1107 - accuracy: 0.9571 - val_loss: 0.0663 - val_accuracy: 0.9764\n",
            "Epoch 103/120\n",
            "125/125 - 0s - loss: 0.1219 - accuracy: 0.9518 - val_loss: 0.0705 - val_accuracy: 0.9764\n",
            "Epoch 104/120\n",
            "125/125 - 0s - loss: 0.1199 - accuracy: 0.9558 - val_loss: 0.0652 - val_accuracy: 0.9782\n",
            "Epoch 105/120\n",
            "125/125 - 0s - loss: 0.1075 - accuracy: 0.9581 - val_loss: 0.0750 - val_accuracy: 0.9791\n",
            "Epoch 106/120\n",
            "125/125 - 0s - loss: 0.1116 - accuracy: 0.9574 - val_loss: 0.0794 - val_accuracy: 0.9710\n",
            "Epoch 107/120\n",
            "125/125 - 0s - loss: 0.1122 - accuracy: 0.9570 - val_loss: 0.0681 - val_accuracy: 0.9773\n",
            "Epoch 108/120\n",
            "125/125 - 0s - loss: 0.1210 - accuracy: 0.9531 - val_loss: 0.0668 - val_accuracy: 0.9810\n",
            "Epoch 109/120\n",
            "125/125 - 0s - loss: 0.1129 - accuracy: 0.9587 - val_loss: 0.0654 - val_accuracy: 0.9773\n",
            "Epoch 110/120\n",
            "125/125 - 0s - loss: 0.1072 - accuracy: 0.9598 - val_loss: 0.0662 - val_accuracy: 0.9782\n",
            "Epoch 111/120\n",
            "125/125 - 0s - loss: 0.1104 - accuracy: 0.9592 - val_loss: 0.0659 - val_accuracy: 0.9773\n",
            "Epoch 112/120\n",
            "125/125 - 0s - loss: 0.1117 - accuracy: 0.9586 - val_loss: 0.0721 - val_accuracy: 0.9782\n",
            "Epoch 113/120\n",
            "125/125 - 0s - loss: 0.1133 - accuracy: 0.9568 - val_loss: 0.0607 - val_accuracy: 0.9791\n",
            "Epoch 114/120\n",
            "125/125 - 0s - loss: 0.1151 - accuracy: 0.9570 - val_loss: 0.0646 - val_accuracy: 0.9764\n",
            "Epoch 115/120\n",
            "125/125 - 0s - loss: 0.1136 - accuracy: 0.9562 - val_loss: 0.0598 - val_accuracy: 0.9773\n",
            "Epoch 116/120\n",
            "125/125 - 0s - loss: 0.1142 - accuracy: 0.9549 - val_loss: 0.0669 - val_accuracy: 0.9764\n",
            "Epoch 117/120\n",
            "125/125 - 0s - loss: 0.1012 - accuracy: 0.9653 - val_loss: 0.0708 - val_accuracy: 0.9710\n",
            "Epoch 118/120\n",
            "125/125 - 0s - loss: 0.1073 - accuracy: 0.9586 - val_loss: 0.0676 - val_accuracy: 0.9773\n",
            "Epoch 119/120\n",
            "125/125 - 0s - loss: 0.1146 - accuracy: 0.9531 - val_loss: 0.0611 - val_accuracy: 0.9782\n",
            "Epoch 120/120\n",
            "125/125 - 0s - loss: 0.1172 - accuracy: 0.9557 - val_loss: 0.0728 - val_accuracy: 0.9737\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUBTtgT0Ah4y",
        "outputId": "b8681ba3-b842-4f35-dbea-dfb9ad197d6e"
      },
      "source": [
        "nn_test_score = model.evaluate(test.values, oh_testing_labels, verbose=2)\n",
        "print(\"Neural Network accuracy of {} on the test set\".format(nn_test_score[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "93/93 - 0s - loss: 0.1654 - accuracy: 0.9569\n",
            "Neural Network accuracy of 0.9569053053855896 on the test set\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "xx6Vi1NVAi6k",
        "outputId": "b4813791-e193-4d58-932e-3de4e1d28879"
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('categorical cross entropy loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['training loss', 'validation loss'], loc = 'upper right' )\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4Tr3huQpI06",
        "outputId": "67149195-50a5-4b31-d8c6-5515febab50b"
      },
      "source": [
        "# Using relu activation function\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense, Dropout\r\n",
        "from keras.optimizers import SGD\r\n",
        "from keras.regularizers import l2\r\n",
        "from keras.callbacks import EarlyStopping\r\n",
        "\r\n",
        "# Build a network for this classification task\r\n",
        "model = Sequential()\r\n",
        "model.add(Dense(96, input_dim = train.shape[1], activation = 'tanh'))\r\n",
        "model.add(Dropout(0.5))\r\n",
        "model.add(Dense(30, activation = 'relu'))\r\n",
        "model.add(Dropout(0.2))\r\n",
        "model.add(Dense(24, activation = 'relu'))\r\n",
        "model.add(Dense(6, activation = 'softmax'))\r\n",
        "\r\n",
        "sgd = SGD(lr = .001, momentum = .9, decay = 4e-3)\r\n",
        "model.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\r\n",
        "\r\n",
        "history = model.fit(train.values, oh_training_labels, epochs = 120, batch_size = 50, verbose = 2,\r\n",
        "          validation_split = .15, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/120\n",
            "125/125 - 1s - loss: 1.5786 - accuracy: 0.3175 - val_loss: 1.1852 - val_accuracy: 0.5539\n",
            "Epoch 2/120\n",
            "125/125 - 0s - loss: 1.2196 - accuracy: 0.4695 - val_loss: 0.9724 - val_accuracy: 0.7353\n",
            "Epoch 3/120\n",
            "125/125 - 0s - loss: 1.0591 - accuracy: 0.5430 - val_loss: 0.8561 - val_accuracy: 0.7425\n",
            "Epoch 4/120\n",
            "125/125 - 0s - loss: 0.9459 - accuracy: 0.5987 - val_loss: 0.7425 - val_accuracy: 0.7724\n",
            "Epoch 5/120\n",
            "125/125 - 0s - loss: 0.8835 - accuracy: 0.6207 - val_loss: 0.6609 - val_accuracy: 0.8051\n",
            "Epoch 6/120\n",
            "125/125 - 0s - loss: 0.8167 - accuracy: 0.6476 - val_loss: 0.6021 - val_accuracy: 0.8123\n",
            "Epoch 7/120\n",
            "125/125 - 0s - loss: 0.7744 - accuracy: 0.6740 - val_loss: 0.5567 - val_accuracy: 0.8114\n",
            "Epoch 8/120\n",
            "125/125 - 0s - loss: 0.7362 - accuracy: 0.6931 - val_loss: 0.5282 - val_accuracy: 0.8005\n",
            "Epoch 9/120\n",
            "125/125 - 0s - loss: 0.7030 - accuracy: 0.7014 - val_loss: 0.5020 - val_accuracy: 0.8015\n",
            "Epoch 10/120\n",
            "125/125 - 0s - loss: 0.6704 - accuracy: 0.7145 - val_loss: 0.4782 - val_accuracy: 0.8495\n",
            "Epoch 11/120\n",
            "125/125 - 0s - loss: 0.6482 - accuracy: 0.7302 - val_loss: 0.4594 - val_accuracy: 0.8740\n",
            "Epoch 12/120\n",
            "125/125 - 0s - loss: 0.6368 - accuracy: 0.7206 - val_loss: 0.4474 - val_accuracy: 0.8722\n",
            "Epoch 13/120\n",
            "125/125 - 0s - loss: 0.6100 - accuracy: 0.7443 - val_loss: 0.4371 - val_accuracy: 0.8758\n",
            "Epoch 14/120\n",
            "125/125 - 0s - loss: 0.6032 - accuracy: 0.7420 - val_loss: 0.4266 - val_accuracy: 0.8803\n",
            "Epoch 15/120\n",
            "125/125 - 0s - loss: 0.5922 - accuracy: 0.7427 - val_loss: 0.4168 - val_accuracy: 0.8785\n",
            "Epoch 16/120\n",
            "125/125 - 0s - loss: 0.5764 - accuracy: 0.7454 - val_loss: 0.4091 - val_accuracy: 0.8749\n",
            "Epoch 17/120\n",
            "125/125 - 0s - loss: 0.5638 - accuracy: 0.7548 - val_loss: 0.4013 - val_accuracy: 0.8776\n",
            "Epoch 18/120\n",
            "125/125 - 0s - loss: 0.5487 - accuracy: 0.7652 - val_loss: 0.3940 - val_accuracy: 0.8731\n",
            "Epoch 19/120\n",
            "125/125 - 0s - loss: 0.5543 - accuracy: 0.7585 - val_loss: 0.3882 - val_accuracy: 0.8749\n",
            "Epoch 20/120\n",
            "125/125 - 0s - loss: 0.5453 - accuracy: 0.7611 - val_loss: 0.3835 - val_accuracy: 0.8749\n",
            "Epoch 21/120\n",
            "125/125 - 0s - loss: 0.5294 - accuracy: 0.7697 - val_loss: 0.3801 - val_accuracy: 0.8840\n",
            "Epoch 22/120\n",
            "125/125 - 0s - loss: 0.5249 - accuracy: 0.7694 - val_loss: 0.3756 - val_accuracy: 0.8758\n",
            "Epoch 23/120\n",
            "125/125 - 0s - loss: 0.5241 - accuracy: 0.7750 - val_loss: 0.3725 - val_accuracy: 0.8803\n",
            "Epoch 24/120\n",
            "125/125 - 0s - loss: 0.5221 - accuracy: 0.7710 - val_loss: 0.3681 - val_accuracy: 0.8776\n",
            "Epoch 25/120\n",
            "125/125 - 0s - loss: 0.5118 - accuracy: 0.7804 - val_loss: 0.3656 - val_accuracy: 0.8849\n",
            "Epoch 26/120\n",
            "125/125 - 0s - loss: 0.5083 - accuracy: 0.7814 - val_loss: 0.3625 - val_accuracy: 0.8758\n",
            "Epoch 27/120\n",
            "125/125 - 0s - loss: 0.5077 - accuracy: 0.7830 - val_loss: 0.3590 - val_accuracy: 0.8794\n",
            "Epoch 28/120\n",
            "125/125 - 0s - loss: 0.5036 - accuracy: 0.7873 - val_loss: 0.3553 - val_accuracy: 0.8849\n",
            "Epoch 29/120\n",
            "125/125 - 0s - loss: 0.5026 - accuracy: 0.7849 - val_loss: 0.3527 - val_accuracy: 0.8767\n",
            "Epoch 30/120\n",
            "125/125 - 0s - loss: 0.5009 - accuracy: 0.7816 - val_loss: 0.3503 - val_accuracy: 0.8858\n",
            "Epoch 31/120\n",
            "125/125 - 0s - loss: 0.4937 - accuracy: 0.7840 - val_loss: 0.3498 - val_accuracy: 0.8858\n",
            "Epoch 32/120\n",
            "125/125 - 0s - loss: 0.4896 - accuracy: 0.7756 - val_loss: 0.3464 - val_accuracy: 0.8858\n",
            "Epoch 33/120\n",
            "125/125 - 0s - loss: 0.4799 - accuracy: 0.7904 - val_loss: 0.3439 - val_accuracy: 0.8794\n",
            "Epoch 34/120\n",
            "125/125 - 0s - loss: 0.4827 - accuracy: 0.7937 - val_loss: 0.3432 - val_accuracy: 0.8776\n",
            "Epoch 35/120\n",
            "125/125 - 0s - loss: 0.4866 - accuracy: 0.7899 - val_loss: 0.3408 - val_accuracy: 0.8794\n",
            "Epoch 36/120\n",
            "125/125 - 0s - loss: 0.4808 - accuracy: 0.7915 - val_loss: 0.3390 - val_accuracy: 0.8794\n",
            "Epoch 37/120\n",
            "125/125 - 0s - loss: 0.4728 - accuracy: 0.7864 - val_loss: 0.3375 - val_accuracy: 0.8830\n",
            "Epoch 38/120\n",
            "125/125 - 0s - loss: 0.4761 - accuracy: 0.7883 - val_loss: 0.3359 - val_accuracy: 0.8776\n",
            "Epoch 39/120\n",
            "125/125 - 0s - loss: 0.4706 - accuracy: 0.7985 - val_loss: 0.3340 - val_accuracy: 0.8794\n",
            "Epoch 40/120\n",
            "125/125 - 0s - loss: 0.4707 - accuracy: 0.7988 - val_loss: 0.3334 - val_accuracy: 0.8785\n",
            "Epoch 41/120\n",
            "125/125 - 0s - loss: 0.4604 - accuracy: 0.8004 - val_loss: 0.3303 - val_accuracy: 0.8830\n",
            "Epoch 42/120\n",
            "125/125 - 0s - loss: 0.4710 - accuracy: 0.7985 - val_loss: 0.3288 - val_accuracy: 0.8858\n",
            "Epoch 43/120\n",
            "125/125 - 0s - loss: 0.4649 - accuracy: 0.7940 - val_loss: 0.3272 - val_accuracy: 0.8830\n",
            "Epoch 44/120\n",
            "125/125 - 0s - loss: 0.4569 - accuracy: 0.8028 - val_loss: 0.3254 - val_accuracy: 0.8894\n",
            "Epoch 45/120\n",
            "125/125 - 0s - loss: 0.4589 - accuracy: 0.8009 - val_loss: 0.3248 - val_accuracy: 0.8821\n",
            "Epoch 46/120\n",
            "125/125 - 0s - loss: 0.4623 - accuracy: 0.7984 - val_loss: 0.3238 - val_accuracy: 0.8858\n",
            "Epoch 47/120\n",
            "125/125 - 0s - loss: 0.4585 - accuracy: 0.7995 - val_loss: 0.3227 - val_accuracy: 0.8858\n",
            "Epoch 48/120\n",
            "125/125 - 0s - loss: 0.4554 - accuracy: 0.8032 - val_loss: 0.3211 - val_accuracy: 0.8830\n",
            "Epoch 49/120\n",
            "125/125 - 0s - loss: 0.4523 - accuracy: 0.8094 - val_loss: 0.3199 - val_accuracy: 0.8867\n",
            "Epoch 50/120\n",
            "125/125 - 0s - loss: 0.4531 - accuracy: 0.8054 - val_loss: 0.3184 - val_accuracy: 0.8885\n",
            "Epoch 51/120\n",
            "125/125 - 0s - loss: 0.4508 - accuracy: 0.8081 - val_loss: 0.3167 - val_accuracy: 0.8858\n",
            "Epoch 52/120\n",
            "125/125 - 0s - loss: 0.4507 - accuracy: 0.8044 - val_loss: 0.3163 - val_accuracy: 0.8894\n",
            "Epoch 53/120\n",
            "125/125 - 0s - loss: 0.4519 - accuracy: 0.8054 - val_loss: 0.3156 - val_accuracy: 0.8867\n",
            "Epoch 54/120\n",
            "125/125 - 0s - loss: 0.4412 - accuracy: 0.8049 - val_loss: 0.3146 - val_accuracy: 0.8867\n",
            "Epoch 55/120\n",
            "125/125 - 0s - loss: 0.4421 - accuracy: 0.8123 - val_loss: 0.3136 - val_accuracy: 0.8858\n",
            "Epoch 56/120\n",
            "125/125 - 0s - loss: 0.4436 - accuracy: 0.8084 - val_loss: 0.3131 - val_accuracy: 0.8894\n",
            "Epoch 57/120\n",
            "125/125 - 0s - loss: 0.4282 - accuracy: 0.8185 - val_loss: 0.3116 - val_accuracy: 0.8867\n",
            "Epoch 58/120\n",
            "125/125 - 0s - loss: 0.4376 - accuracy: 0.8094 - val_loss: 0.3108 - val_accuracy: 0.8858\n",
            "Epoch 59/120\n",
            "125/125 - 0s - loss: 0.4481 - accuracy: 0.8065 - val_loss: 0.3098 - val_accuracy: 0.8876\n",
            "Epoch 60/120\n",
            "125/125 - 0s - loss: 0.4417 - accuracy: 0.8113 - val_loss: 0.3092 - val_accuracy: 0.8885\n",
            "Epoch 61/120\n",
            "125/125 - 0s - loss: 0.4334 - accuracy: 0.8124 - val_loss: 0.3087 - val_accuracy: 0.8867\n",
            "Epoch 62/120\n",
            "125/125 - 0s - loss: 0.4353 - accuracy: 0.8107 - val_loss: 0.3075 - val_accuracy: 0.8894\n",
            "Epoch 63/120\n",
            "125/125 - 0s - loss: 0.4392 - accuracy: 0.8081 - val_loss: 0.3071 - val_accuracy: 0.8912\n",
            "Epoch 64/120\n",
            "125/125 - 0s - loss: 0.4320 - accuracy: 0.8129 - val_loss: 0.3065 - val_accuracy: 0.8885\n",
            "Epoch 65/120\n",
            "125/125 - 0s - loss: 0.4272 - accuracy: 0.8168 - val_loss: 0.3060 - val_accuracy: 0.8894\n",
            "Epoch 66/120\n",
            "125/125 - 0s - loss: 0.4256 - accuracy: 0.8157 - val_loss: 0.3048 - val_accuracy: 0.8903\n",
            "Epoch 67/120\n",
            "125/125 - 0s - loss: 0.4254 - accuracy: 0.8131 - val_loss: 0.3035 - val_accuracy: 0.8894\n",
            "Epoch 68/120\n",
            "125/125 - 0s - loss: 0.4371 - accuracy: 0.8137 - val_loss: 0.3032 - val_accuracy: 0.8894\n",
            "Epoch 69/120\n",
            "125/125 - 0s - loss: 0.4279 - accuracy: 0.8179 - val_loss: 0.3024 - val_accuracy: 0.8912\n",
            "Epoch 70/120\n",
            "125/125 - 0s - loss: 0.4277 - accuracy: 0.8137 - val_loss: 0.3020 - val_accuracy: 0.8894\n",
            "Epoch 71/120\n",
            "125/125 - 0s - loss: 0.4271 - accuracy: 0.8129 - val_loss: 0.3003 - val_accuracy: 0.8948\n",
            "Epoch 72/120\n",
            "125/125 - 0s - loss: 0.4290 - accuracy: 0.8171 - val_loss: 0.3001 - val_accuracy: 0.8912\n",
            "Epoch 73/120\n",
            "125/125 - 0s - loss: 0.4218 - accuracy: 0.8201 - val_loss: 0.2989 - val_accuracy: 0.8921\n",
            "Epoch 74/120\n",
            "125/125 - 0s - loss: 0.4180 - accuracy: 0.8141 - val_loss: 0.2985 - val_accuracy: 0.8939\n",
            "Epoch 75/120\n",
            "125/125 - 0s - loss: 0.4198 - accuracy: 0.8209 - val_loss: 0.2979 - val_accuracy: 0.8939\n",
            "Epoch 76/120\n",
            "125/125 - 0s - loss: 0.4245 - accuracy: 0.8157 - val_loss: 0.2971 - val_accuracy: 0.8939\n",
            "Epoch 77/120\n",
            "125/125 - 0s - loss: 0.4216 - accuracy: 0.8152 - val_loss: 0.2966 - val_accuracy: 0.8921\n",
            "Epoch 78/120\n",
            "125/125 - 0s - loss: 0.4226 - accuracy: 0.8160 - val_loss: 0.2962 - val_accuracy: 0.8912\n",
            "Epoch 79/120\n",
            "125/125 - 0s - loss: 0.4174 - accuracy: 0.8229 - val_loss: 0.2955 - val_accuracy: 0.8930\n",
            "Epoch 80/120\n",
            "125/125 - 0s - loss: 0.4187 - accuracy: 0.8155 - val_loss: 0.2952 - val_accuracy: 0.8921\n",
            "Epoch 81/120\n",
            "125/125 - 0s - loss: 0.4168 - accuracy: 0.8160 - val_loss: 0.2947 - val_accuracy: 0.8921\n",
            "Epoch 82/120\n",
            "125/125 - 0s - loss: 0.4207 - accuracy: 0.8216 - val_loss: 0.2942 - val_accuracy: 0.8939\n",
            "Epoch 83/120\n",
            "125/125 - 0s - loss: 0.4127 - accuracy: 0.8225 - val_loss: 0.2938 - val_accuracy: 0.8948\n",
            "Epoch 84/120\n",
            "125/125 - 0s - loss: 0.4204 - accuracy: 0.8200 - val_loss: 0.2934 - val_accuracy: 0.8939\n",
            "Epoch 85/120\n",
            "125/125 - 0s - loss: 0.4177 - accuracy: 0.8203 - val_loss: 0.2932 - val_accuracy: 0.8939\n",
            "Epoch 86/120\n",
            "125/125 - 0s - loss: 0.4148 - accuracy: 0.8200 - val_loss: 0.2926 - val_accuracy: 0.8939\n",
            "Epoch 87/120\n",
            "125/125 - 0s - loss: 0.4091 - accuracy: 0.8278 - val_loss: 0.2921 - val_accuracy: 0.8912\n",
            "Epoch 88/120\n",
            "125/125 - 0s - loss: 0.4085 - accuracy: 0.8248 - val_loss: 0.2909 - val_accuracy: 0.8948\n",
            "Epoch 89/120\n",
            "125/125 - 0s - loss: 0.4109 - accuracy: 0.8195 - val_loss: 0.2902 - val_accuracy: 0.8939\n",
            "Epoch 90/120\n",
            "125/125 - 0s - loss: 0.4064 - accuracy: 0.8248 - val_loss: 0.2897 - val_accuracy: 0.8957\n",
            "Epoch 91/120\n",
            "125/125 - 0s - loss: 0.4018 - accuracy: 0.8217 - val_loss: 0.2891 - val_accuracy: 0.8957\n",
            "Epoch 92/120\n",
            "125/125 - 0s - loss: 0.4086 - accuracy: 0.8337 - val_loss: 0.2882 - val_accuracy: 0.8957\n",
            "Epoch 93/120\n",
            "125/125 - 0s - loss: 0.4163 - accuracy: 0.8206 - val_loss: 0.2879 - val_accuracy: 0.8976\n",
            "Epoch 94/120\n",
            "125/125 - 0s - loss: 0.4156 - accuracy: 0.8240 - val_loss: 0.2875 - val_accuracy: 0.8948\n",
            "Epoch 95/120\n",
            "125/125 - 0s - loss: 0.4063 - accuracy: 0.8245 - val_loss: 0.2872 - val_accuracy: 0.8948\n",
            "Epoch 96/120\n",
            "125/125 - 0s - loss: 0.4033 - accuracy: 0.8286 - val_loss: 0.2869 - val_accuracy: 0.8948\n",
            "Epoch 97/120\n",
            "125/125 - 0s - loss: 0.4056 - accuracy: 0.8291 - val_loss: 0.2861 - val_accuracy: 0.8948\n",
            "Epoch 98/120\n",
            "125/125 - 0s - loss: 0.4012 - accuracy: 0.8336 - val_loss: 0.2860 - val_accuracy: 0.8948\n",
            "Epoch 99/120\n",
            "125/125 - 0s - loss: 0.3984 - accuracy: 0.8310 - val_loss: 0.2850 - val_accuracy: 0.8948\n",
            "Epoch 100/120\n",
            "125/125 - 0s - loss: 0.4059 - accuracy: 0.8257 - val_loss: 0.2842 - val_accuracy: 0.8948\n",
            "Epoch 101/120\n",
            "125/125 - 0s - loss: 0.4020 - accuracy: 0.8270 - val_loss: 0.2838 - val_accuracy: 0.8957\n",
            "Epoch 102/120\n",
            "125/125 - 0s - loss: 0.4084 - accuracy: 0.8254 - val_loss: 0.2832 - val_accuracy: 0.8957\n",
            "Epoch 103/120\n",
            "125/125 - 0s - loss: 0.4117 - accuracy: 0.8246 - val_loss: 0.2834 - val_accuracy: 0.8966\n",
            "Epoch 104/120\n",
            "125/125 - 0s - loss: 0.4027 - accuracy: 0.8259 - val_loss: 0.2829 - val_accuracy: 0.8966\n",
            "Epoch 105/120\n",
            "125/125 - 0s - loss: 0.4054 - accuracy: 0.8256 - val_loss: 0.2830 - val_accuracy: 0.8939\n",
            "Epoch 106/120\n",
            "125/125 - 0s - loss: 0.4013 - accuracy: 0.8309 - val_loss: 0.2822 - val_accuracy: 0.8976\n",
            "Epoch 107/120\n",
            "125/125 - 0s - loss: 0.4086 - accuracy: 0.8182 - val_loss: 0.2815 - val_accuracy: 0.8976\n",
            "Epoch 108/120\n",
            "125/125 - 0s - loss: 0.3988 - accuracy: 0.8293 - val_loss: 0.2815 - val_accuracy: 0.8966\n",
            "Epoch 109/120\n",
            "125/125 - 0s - loss: 0.4060 - accuracy: 0.8225 - val_loss: 0.2813 - val_accuracy: 0.8966\n",
            "Epoch 110/120\n",
            "125/125 - 0s - loss: 0.4048 - accuracy: 0.8253 - val_loss: 0.2809 - val_accuracy: 0.8985\n",
            "Epoch 111/120\n",
            "125/125 - 0s - loss: 0.3932 - accuracy: 0.8323 - val_loss: 0.2803 - val_accuracy: 0.8994\n",
            "Epoch 112/120\n",
            "125/125 - 0s - loss: 0.3972 - accuracy: 0.8296 - val_loss: 0.2798 - val_accuracy: 0.8976\n",
            "Epoch 113/120\n",
            "125/125 - 0s - loss: 0.3969 - accuracy: 0.8310 - val_loss: 0.2794 - val_accuracy: 0.8994\n",
            "Epoch 114/120\n",
            "125/125 - 0s - loss: 0.4071 - accuracy: 0.8281 - val_loss: 0.2789 - val_accuracy: 0.8994\n",
            "Epoch 115/120\n",
            "125/125 - 0s - loss: 0.3931 - accuracy: 0.8307 - val_loss: 0.2783 - val_accuracy: 0.8994\n",
            "Epoch 116/120\n",
            "125/125 - 0s - loss: 0.3968 - accuracy: 0.8342 - val_loss: 0.2784 - val_accuracy: 0.8994\n",
            "Epoch 117/120\n",
            "125/125 - 0s - loss: 0.4021 - accuracy: 0.8305 - val_loss: 0.2783 - val_accuracy: 0.8976\n",
            "Epoch 118/120\n",
            "125/125 - 0s - loss: 0.3916 - accuracy: 0.8342 - val_loss: 0.2776 - val_accuracy: 0.8985\n",
            "Epoch 119/120\n",
            "125/125 - 0s - loss: 0.4012 - accuracy: 0.8240 - val_loss: 0.2774 - val_accuracy: 0.8994\n",
            "Epoch 120/120\n",
            "125/125 - 0s - loss: 0.3895 - accuracy: 0.8363 - val_loss: 0.2766 - val_accuracy: 0.8985\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnJGWj3RpMPg",
        "outputId": "39c9cdd3-593f-4595-fc93-3c99b7c3f4d5"
      },
      "source": [
        "nn_test_score = model.evaluate(test.values, oh_testing_labels, verbose=2)\r\n",
        "print(\"Neural Network accuracy of {} on the test set\".format(nn_test_score[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "93/93 - 0s - loss: 0.3175 - accuracy: 0.8704\n",
            "Neural Network accuracy of 0.8703766465187073 on the test set\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2DpSPItpZEh",
        "outputId": "9ef663ad-b55b-4454-de81-4ebc33a2919c"
      },
      "source": [
        "# Using relu activation function\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense, Dropout\r\n",
        "from keras.optimizers import SGD\r\n",
        "from keras.regularizers import l2\r\n",
        "from keras.callbacks import EarlyStopping\r\n",
        "\r\n",
        "# Build a network for this classification task\r\n",
        "model = Sequential()\r\n",
        "model.add(Dense(96, input_dim = train.shape[1], activation = 'tanh'))\r\n",
        "model.add(Dropout(0.5))\r\n",
        "model.add(Dense(30, activation = 'relu'))\r\n",
        "model.add(Dropout(0.2))\r\n",
        "model.add(Dense(24, activation = 'relu'))\r\n",
        "model.add(Dense(6, activation = 'softmax'))\r\n",
        "\r\n",
        "sgd = SGD(lr = .1, momentum = .9, decay = 4e-3)\r\n",
        "model.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\r\n",
        "\r\n",
        "history = model.fit(train.values, oh_training_labels, epochs = 120, batch_size = 50, verbose = 2,\r\n",
        "          validation_split = .15, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/120\n",
            "125/125 - 1s - loss: 1.2425 - accuracy: 0.3671 - val_loss: 1.0319 - val_accuracy: 0.4687\n",
            "Epoch 2/120\n",
            "125/125 - 0s - loss: 0.8623 - accuracy: 0.5522 - val_loss: 0.5880 - val_accuracy: 0.6301\n",
            "Epoch 3/120\n",
            "125/125 - 0s - loss: 0.7129 - accuracy: 0.6067 - val_loss: 0.5572 - val_accuracy: 0.6256\n",
            "Epoch 4/120\n",
            "125/125 - 0s - loss: 0.6383 - accuracy: 0.6303 - val_loss: 0.5070 - val_accuracy: 0.6618\n",
            "Epoch 5/120\n",
            "125/125 - 0s - loss: 0.6141 - accuracy: 0.6342 - val_loss: 0.5066 - val_accuracy: 0.6519\n",
            "Epoch 6/120\n",
            "125/125 - 0s - loss: 0.5625 - accuracy: 0.6563 - val_loss: 0.5208 - val_accuracy: 0.6664\n",
            "Epoch 7/120\n",
            "125/125 - 0s - loss: 0.5448 - accuracy: 0.6683 - val_loss: 0.4630 - val_accuracy: 0.7316\n",
            "Epoch 8/120\n",
            "125/125 - 0s - loss: 0.5084 - accuracy: 0.6968 - val_loss: 0.3891 - val_accuracy: 0.7842\n",
            "Epoch 9/120\n",
            "125/125 - 0s - loss: 0.4640 - accuracy: 0.7408 - val_loss: 0.3364 - val_accuracy: 0.8105\n",
            "Epoch 10/120\n",
            "125/125 - 0s - loss: 0.4158 - accuracy: 0.7702 - val_loss: 0.3596 - val_accuracy: 0.7743\n",
            "Epoch 11/120\n",
            "125/125 - 0s - loss: 0.3885 - accuracy: 0.7846 - val_loss: 0.2815 - val_accuracy: 0.8277\n",
            "Epoch 12/120\n",
            "125/125 - 0s - loss: 0.3651 - accuracy: 0.7883 - val_loss: 0.2740 - val_accuracy: 0.8867\n",
            "Epoch 13/120\n",
            "125/125 - 0s - loss: 0.3475 - accuracy: 0.7948 - val_loss: 0.2590 - val_accuracy: 0.8586\n",
            "Epoch 14/120\n",
            "125/125 - 0s - loss: 0.3356 - accuracy: 0.8110 - val_loss: 0.2546 - val_accuracy: 0.8422\n",
            "Epoch 15/120\n",
            "125/125 - 0s - loss: 0.3304 - accuracy: 0.8129 - val_loss: 0.2599 - val_accuracy: 0.8658\n",
            "Epoch 16/120\n",
            "125/125 - 0s - loss: 0.3258 - accuracy: 0.8201 - val_loss: 0.2485 - val_accuracy: 0.8966\n",
            "Epoch 17/120\n",
            "125/125 - 0s - loss: 0.3137 - accuracy: 0.8347 - val_loss: 0.2507 - val_accuracy: 0.8477\n",
            "Epoch 18/120\n",
            "125/125 - 0s - loss: 0.2969 - accuracy: 0.8441 - val_loss: 0.2190 - val_accuracy: 0.9175\n",
            "Epoch 19/120\n",
            "125/125 - 0s - loss: 0.2876 - accuracy: 0.8528 - val_loss: 0.2214 - val_accuracy: 0.8921\n",
            "Epoch 20/120\n",
            "125/125 - 0s - loss: 0.2869 - accuracy: 0.8601 - val_loss: 0.2156 - val_accuracy: 0.8731\n",
            "Epoch 21/120\n",
            "125/125 - 0s - loss: 0.2655 - accuracy: 0.8699 - val_loss: 0.1802 - val_accuracy: 0.9229\n",
            "Epoch 22/120\n",
            "125/125 - 0s - loss: 0.2441 - accuracy: 0.8822 - val_loss: 0.1754 - val_accuracy: 0.9374\n",
            "Epoch 23/120\n",
            "125/125 - 0s - loss: 0.2424 - accuracy: 0.8889 - val_loss: 0.1663 - val_accuracy: 0.9302\n",
            "Epoch 24/120\n",
            "125/125 - 0s - loss: 0.2406 - accuracy: 0.8862 - val_loss: 0.1518 - val_accuracy: 0.9393\n",
            "Epoch 25/120\n",
            "125/125 - 0s - loss: 0.2359 - accuracy: 0.8917 - val_loss: 0.1549 - val_accuracy: 0.9465\n",
            "Epoch 26/120\n",
            "125/125 - 0s - loss: 0.2290 - accuracy: 0.8949 - val_loss: 0.1529 - val_accuracy: 0.9456\n",
            "Epoch 27/120\n",
            "125/125 - 0s - loss: 0.2153 - accuracy: 0.9025 - val_loss: 0.1470 - val_accuracy: 0.9429\n",
            "Epoch 28/120\n",
            "125/125 - 0s - loss: 0.2158 - accuracy: 0.9045 - val_loss: 0.1345 - val_accuracy: 0.9447\n",
            "Epoch 29/120\n",
            "125/125 - 0s - loss: 0.2121 - accuracy: 0.9085 - val_loss: 0.1362 - val_accuracy: 0.9456\n",
            "Epoch 30/120\n",
            "125/125 - 0s - loss: 0.2018 - accuracy: 0.9097 - val_loss: 0.1221 - val_accuracy: 0.9519\n",
            "Epoch 31/120\n",
            "125/125 - 0s - loss: 0.1848 - accuracy: 0.9201 - val_loss: 0.1203 - val_accuracy: 0.9501\n",
            "Epoch 32/120\n",
            "125/125 - 0s - loss: 0.1897 - accuracy: 0.9259 - val_loss: 0.1214 - val_accuracy: 0.9519\n",
            "Epoch 33/120\n",
            "125/125 - 0s - loss: 0.1788 - accuracy: 0.9262 - val_loss: 0.1032 - val_accuracy: 0.9619\n",
            "Epoch 34/120\n",
            "125/125 - 0s - loss: 0.1796 - accuracy: 0.9213 - val_loss: 0.0998 - val_accuracy: 0.9619\n",
            "Epoch 35/120\n",
            "125/125 - 0s - loss: 0.1825 - accuracy: 0.9273 - val_loss: 0.1071 - val_accuracy: 0.9646\n",
            "Epoch 36/120\n",
            "125/125 - 0s - loss: 0.1730 - accuracy: 0.9293 - val_loss: 0.1488 - val_accuracy: 0.9329\n",
            "Epoch 37/120\n",
            "125/125 - 0s - loss: 0.1623 - accuracy: 0.9344 - val_loss: 0.1090 - val_accuracy: 0.9565\n",
            "Epoch 38/120\n",
            "125/125 - 0s - loss: 0.1663 - accuracy: 0.9342 - val_loss: 0.0976 - val_accuracy: 0.9692\n",
            "Epoch 39/120\n",
            "125/125 - 0s - loss: 0.1599 - accuracy: 0.9360 - val_loss: 0.0855 - val_accuracy: 0.9710\n",
            "Epoch 40/120\n",
            "125/125 - 0s - loss: 0.1516 - accuracy: 0.9406 - val_loss: 0.0902 - val_accuracy: 0.9674\n",
            "Epoch 41/120\n",
            "125/125 - 0s - loss: 0.1585 - accuracy: 0.9350 - val_loss: 0.0893 - val_accuracy: 0.9646\n",
            "Epoch 42/120\n",
            "125/125 - 0s - loss: 0.1669 - accuracy: 0.9350 - val_loss: 0.0823 - val_accuracy: 0.9737\n",
            "Epoch 43/120\n",
            "125/125 - 0s - loss: 0.1549 - accuracy: 0.9361 - val_loss: 0.0850 - val_accuracy: 0.9701\n",
            "Epoch 44/120\n",
            "125/125 - 0s - loss: 0.1505 - accuracy: 0.9432 - val_loss: 0.0817 - val_accuracy: 0.9692\n",
            "Epoch 45/120\n",
            "125/125 - 0s - loss: 0.1433 - accuracy: 0.9430 - val_loss: 0.0870 - val_accuracy: 0.9692\n",
            "Epoch 46/120\n",
            "125/125 - 0s - loss: 0.1442 - accuracy: 0.9395 - val_loss: 0.0793 - val_accuracy: 0.9719\n",
            "Epoch 47/120\n",
            "125/125 - 0s - loss: 0.1329 - accuracy: 0.9469 - val_loss: 0.0798 - val_accuracy: 0.9710\n",
            "Epoch 48/120\n",
            "125/125 - 0s - loss: 0.1497 - accuracy: 0.9403 - val_loss: 0.0914 - val_accuracy: 0.9674\n",
            "Epoch 49/120\n",
            "125/125 - 0s - loss: 0.1327 - accuracy: 0.9493 - val_loss: 0.0850 - val_accuracy: 0.9737\n",
            "Epoch 50/120\n",
            "125/125 - 0s - loss: 0.1378 - accuracy: 0.9461 - val_loss: 0.1391 - val_accuracy: 0.9465\n",
            "Epoch 51/120\n",
            "125/125 - 0s - loss: 0.1320 - accuracy: 0.9498 - val_loss: 0.0885 - val_accuracy: 0.9692\n",
            "Epoch 52/120\n",
            "125/125 - 0s - loss: 0.1422 - accuracy: 0.9437 - val_loss: 0.0853 - val_accuracy: 0.9674\n",
            "Epoch 53/120\n",
            "125/125 - 0s - loss: 0.1386 - accuracy: 0.9432 - val_loss: 0.1002 - val_accuracy: 0.9592\n",
            "Epoch 54/120\n",
            "125/125 - 0s - loss: 0.1309 - accuracy: 0.9466 - val_loss: 0.0862 - val_accuracy: 0.9646\n",
            "Epoch 55/120\n",
            "125/125 - 0s - loss: 0.1276 - accuracy: 0.9510 - val_loss: 0.0736 - val_accuracy: 0.9755\n",
            "Epoch 56/120\n",
            "125/125 - 0s - loss: 0.1120 - accuracy: 0.9550 - val_loss: 0.0754 - val_accuracy: 0.9737\n",
            "Epoch 57/120\n",
            "125/125 - 0s - loss: 0.1314 - accuracy: 0.9512 - val_loss: 0.0843 - val_accuracy: 0.9692\n",
            "Epoch 58/120\n",
            "125/125 - 0s - loss: 0.1256 - accuracy: 0.9501 - val_loss: 0.0880 - val_accuracy: 0.9665\n",
            "Epoch 59/120\n",
            "125/125 - 0s - loss: 0.1251 - accuracy: 0.9517 - val_loss: 0.0713 - val_accuracy: 0.9746\n",
            "Epoch 60/120\n",
            "125/125 - 0s - loss: 0.1241 - accuracy: 0.9531 - val_loss: 0.0757 - val_accuracy: 0.9728\n",
            "Epoch 61/120\n",
            "125/125 - 0s - loss: 0.1265 - accuracy: 0.9525 - val_loss: 0.0925 - val_accuracy: 0.9646\n",
            "Epoch 62/120\n",
            "125/125 - 0s - loss: 0.1155 - accuracy: 0.9552 - val_loss: 0.0701 - val_accuracy: 0.9782\n",
            "Epoch 63/120\n",
            "125/125 - 0s - loss: 0.1212 - accuracy: 0.9522 - val_loss: 0.0707 - val_accuracy: 0.9737\n",
            "Epoch 64/120\n",
            "125/125 - 0s - loss: 0.1202 - accuracy: 0.9554 - val_loss: 0.0667 - val_accuracy: 0.9755\n",
            "Epoch 65/120\n",
            "125/125 - 0s - loss: 0.1112 - accuracy: 0.9594 - val_loss: 0.0695 - val_accuracy: 0.9764\n",
            "Epoch 66/120\n",
            "125/125 - 0s - loss: 0.1124 - accuracy: 0.9558 - val_loss: 0.0652 - val_accuracy: 0.9755\n",
            "Epoch 67/120\n",
            "125/125 - 0s - loss: 0.1093 - accuracy: 0.9610 - val_loss: 0.0710 - val_accuracy: 0.9737\n",
            "Epoch 68/120\n",
            "125/125 - 0s - loss: 0.1147 - accuracy: 0.9549 - val_loss: 0.0674 - val_accuracy: 0.9782\n",
            "Epoch 69/120\n",
            "125/125 - 0s - loss: 0.1126 - accuracy: 0.9563 - val_loss: 0.0804 - val_accuracy: 0.9701\n",
            "Epoch 70/120\n",
            "125/125 - 0s - loss: 0.1107 - accuracy: 0.9595 - val_loss: 0.0696 - val_accuracy: 0.9773\n",
            "Epoch 71/120\n",
            "125/125 - 0s - loss: 0.1092 - accuracy: 0.9606 - val_loss: 0.0651 - val_accuracy: 0.9773\n",
            "Epoch 72/120\n",
            "125/125 - 0s - loss: 0.1105 - accuracy: 0.9566 - val_loss: 0.0707 - val_accuracy: 0.9755\n",
            "Epoch 73/120\n",
            "125/125 - 0s - loss: 0.1091 - accuracy: 0.9579 - val_loss: 0.0718 - val_accuracy: 0.9728\n",
            "Epoch 74/120\n",
            "125/125 - 0s - loss: 0.1029 - accuracy: 0.9613 - val_loss: 0.0665 - val_accuracy: 0.9746\n",
            "Epoch 75/120\n",
            "125/125 - 0s - loss: 0.1105 - accuracy: 0.9595 - val_loss: 0.0657 - val_accuracy: 0.9764\n",
            "Epoch 76/120\n",
            "125/125 - 0s - loss: 0.1085 - accuracy: 0.9581 - val_loss: 0.0639 - val_accuracy: 0.9764\n",
            "Epoch 77/120\n",
            "125/125 - 0s - loss: 0.1050 - accuracy: 0.9603 - val_loss: 0.0721 - val_accuracy: 0.9701\n",
            "Epoch 78/120\n",
            "125/125 - 0s - loss: 0.1088 - accuracy: 0.9586 - val_loss: 0.0746 - val_accuracy: 0.9719\n",
            "Epoch 79/120\n",
            "125/125 - 0s - loss: 0.1090 - accuracy: 0.9584 - val_loss: 0.0630 - val_accuracy: 0.9755\n",
            "Epoch 80/120\n",
            "125/125 - 0s - loss: 0.1049 - accuracy: 0.9610 - val_loss: 0.0679 - val_accuracy: 0.9764\n",
            "Epoch 81/120\n",
            "125/125 - 0s - loss: 0.0955 - accuracy: 0.9629 - val_loss: 0.0652 - val_accuracy: 0.9764\n",
            "Epoch 82/120\n",
            "125/125 - 0s - loss: 0.1127 - accuracy: 0.9563 - val_loss: 0.0808 - val_accuracy: 0.9701\n",
            "Epoch 83/120\n",
            "125/125 - 0s - loss: 0.1015 - accuracy: 0.9626 - val_loss: 0.0666 - val_accuracy: 0.9764\n",
            "Epoch 84/120\n",
            "125/125 - 0s - loss: 0.1002 - accuracy: 0.9598 - val_loss: 0.0617 - val_accuracy: 0.9764\n",
            "Epoch 85/120\n",
            "125/125 - 0s - loss: 0.1011 - accuracy: 0.9632 - val_loss: 0.0679 - val_accuracy: 0.9737\n",
            "Epoch 86/120\n",
            "125/125 - 0s - loss: 0.1138 - accuracy: 0.9563 - val_loss: 0.0658 - val_accuracy: 0.9728\n",
            "Epoch 87/120\n",
            "125/125 - 0s - loss: 0.1011 - accuracy: 0.9626 - val_loss: 0.0598 - val_accuracy: 0.9764\n",
            "Epoch 88/120\n",
            "125/125 - 0s - loss: 0.0943 - accuracy: 0.9650 - val_loss: 0.0627 - val_accuracy: 0.9782\n",
            "Epoch 89/120\n",
            "125/125 - 0s - loss: 0.1024 - accuracy: 0.9618 - val_loss: 0.0617 - val_accuracy: 0.9764\n",
            "Epoch 90/120\n",
            "125/125 - 0s - loss: 0.0984 - accuracy: 0.9614 - val_loss: 0.0603 - val_accuracy: 0.9764\n",
            "Epoch 91/120\n",
            "125/125 - 0s - loss: 0.0958 - accuracy: 0.9653 - val_loss: 0.0596 - val_accuracy: 0.9764\n",
            "Epoch 92/120\n",
            "125/125 - 0s - loss: 0.0932 - accuracy: 0.9643 - val_loss: 0.0759 - val_accuracy: 0.9710\n",
            "Epoch 93/120\n",
            "125/125 - 0s - loss: 0.0994 - accuracy: 0.9619 - val_loss: 0.0648 - val_accuracy: 0.9791\n",
            "Epoch 94/120\n",
            "125/125 - 0s - loss: 0.0979 - accuracy: 0.9622 - val_loss: 0.0599 - val_accuracy: 0.9755\n",
            "Epoch 95/120\n",
            "125/125 - 0s - loss: 0.0924 - accuracy: 0.9637 - val_loss: 0.0689 - val_accuracy: 0.9764\n",
            "Epoch 96/120\n",
            "125/125 - 0s - loss: 0.1020 - accuracy: 0.9616 - val_loss: 0.0601 - val_accuracy: 0.9764\n",
            "Epoch 97/120\n",
            "125/125 - 0s - loss: 0.0989 - accuracy: 0.9640 - val_loss: 0.1028 - val_accuracy: 0.9619\n",
            "Epoch 98/120\n",
            "125/125 - 0s - loss: 0.0944 - accuracy: 0.9640 - val_loss: 0.0702 - val_accuracy: 0.9737\n",
            "Epoch 99/120\n",
            "125/125 - 0s - loss: 0.0944 - accuracy: 0.9662 - val_loss: 0.0713 - val_accuracy: 0.9710\n",
            "Epoch 100/120\n",
            "125/125 - 0s - loss: 0.0943 - accuracy: 0.9650 - val_loss: 0.0641 - val_accuracy: 0.9764\n",
            "Epoch 101/120\n",
            "125/125 - 0s - loss: 0.0918 - accuracy: 0.9658 - val_loss: 0.0607 - val_accuracy: 0.9773\n",
            "Epoch 102/120\n",
            "125/125 - 0s - loss: 0.0922 - accuracy: 0.9654 - val_loss: 0.0744 - val_accuracy: 0.9701\n",
            "Epoch 103/120\n",
            "125/125 - 0s - loss: 0.0941 - accuracy: 0.9658 - val_loss: 0.0607 - val_accuracy: 0.9773\n",
            "Epoch 104/120\n",
            "125/125 - 0s - loss: 0.1000 - accuracy: 0.9646 - val_loss: 0.0647 - val_accuracy: 0.9773\n",
            "Epoch 105/120\n",
            "125/125 - 0s - loss: 0.0893 - accuracy: 0.9698 - val_loss: 0.0631 - val_accuracy: 0.9764\n",
            "Epoch 106/120\n",
            "125/125 - 0s - loss: 0.0928 - accuracy: 0.9659 - val_loss: 0.0703 - val_accuracy: 0.9755\n",
            "Epoch 107/120\n",
            "125/125 - 0s - loss: 0.0849 - accuracy: 0.9688 - val_loss: 0.0589 - val_accuracy: 0.9773\n",
            "Epoch 108/120\n",
            "125/125 - 0s - loss: 0.0865 - accuracy: 0.9696 - val_loss: 0.0611 - val_accuracy: 0.9755\n",
            "Epoch 109/120\n",
            "125/125 - 0s - loss: 0.0915 - accuracy: 0.9667 - val_loss: 0.0597 - val_accuracy: 0.9773\n",
            "Epoch 110/120\n",
            "125/125 - 0s - loss: 0.0982 - accuracy: 0.9629 - val_loss: 0.0619 - val_accuracy: 0.9755\n",
            "Epoch 111/120\n",
            "125/125 - 0s - loss: 0.0858 - accuracy: 0.9701 - val_loss: 0.0591 - val_accuracy: 0.9782\n",
            "Epoch 112/120\n",
            "125/125 - 0s - loss: 0.0848 - accuracy: 0.9682 - val_loss: 0.0630 - val_accuracy: 0.9782\n",
            "Epoch 113/120\n",
            "125/125 - 0s - loss: 0.0986 - accuracy: 0.9619 - val_loss: 0.0633 - val_accuracy: 0.9746\n",
            "Epoch 114/120\n",
            "125/125 - 0s - loss: 0.0914 - accuracy: 0.9674 - val_loss: 0.0662 - val_accuracy: 0.9764\n",
            "Epoch 115/120\n",
            "125/125 - 0s - loss: 0.0882 - accuracy: 0.9677 - val_loss: 0.0647 - val_accuracy: 0.9746\n",
            "Epoch 116/120\n",
            "125/125 - 0s - loss: 0.0888 - accuracy: 0.9677 - val_loss: 0.0579 - val_accuracy: 0.9782\n",
            "Epoch 117/120\n",
            "125/125 - 0s - loss: 0.0854 - accuracy: 0.9683 - val_loss: 0.0557 - val_accuracy: 0.9782\n",
            "Epoch 118/120\n",
            "125/125 - 0s - loss: 0.0817 - accuracy: 0.9677 - val_loss: 0.0847 - val_accuracy: 0.9701\n",
            "Epoch 119/120\n",
            "125/125 - 0s - loss: 0.0945 - accuracy: 0.9624 - val_loss: 0.0602 - val_accuracy: 0.9782\n",
            "Epoch 120/120\n",
            "125/125 - 0s - loss: 0.0878 - accuracy: 0.9698 - val_loss: 0.0587 - val_accuracy: 0.9764\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnMsV3fypdYH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}